{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186f5a6-dcf3-4a61-9301-4fb2a46810c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- 1) Paths & config ----------\n",
    "INPUT_DIR  = Path(r\"C:\\temp\\timor_leste\\ndvi_evi\")\n",
    "OUTPUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "PLOTS_DIR  = OUTPUT_DIR / \"plots\"\n",
    "\n",
    "# Plot subfolders\n",
    "PLOT_DIR_AUC_NDVI        = PLOTS_DIR / \"auc_ndvi\"\n",
    "PLOT_DIR_AUC_EVI         = PLOTS_DIR / \"auc_evi\"\n",
    "PLOT_DIR_HARV_MEAN_NDVI  = PLOTS_DIR / \"harvest_mean_ndvi\"\n",
    "PLOT_DIR_HARV_MEAN_EVI   = PLOTS_DIR / \"harvest_mean_evi\"\n",
    "\n",
    "# Seasonal overlay / time-series dirs (estimated harvest windows)\n",
    "OVERLAY_NDVI_DIR   = PLOTS_DIR / \"seasonal_overlay_ndvi_estharvest\"\n",
    "OVERLAY_EVI_DIR    = PLOTS_DIR / \"seasonal_overlay_evi_estharvest\"\n",
    "TS_NDVI_DIR        = PLOTS_DIR / \"timeseries_ndvi_estharvest\"\n",
    "TS_EVI_DIR         = PLOTS_DIR / \"timeseries_evi_estharvest\"\n",
    "\n",
    "# Make dirs once\n",
    "for d in [OUTPUT_DIR, PLOTS_DIR, PLOT_DIR_AUC_NDVI, PLOT_DIR_AUC_EVI,\n",
    "          PLOT_DIR_HARV_MEAN_NDVI, PLOT_DIR_HARV_MEAN_EVI,\n",
    "          OVERLAY_NDVI_DIR, OVERLAY_EVI_DIR, TS_NDVI_DIR, TS_EVI_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Global analysis config\n",
    "BASELINE_YEARS  = [2019, 2020, 2021]\n",
    "ANALYSIS_YEARS  = [2022, 2023, 2024, 2025]\n",
    "HARVEST_WIN_CSV = OUTPUT_DIR / \"estimated_harvest_windows.csv\"\n",
    "USE_CONSENSUS_FOR_BOTH = False\n",
    "\n",
    "pd.set_option(\"display.width\", 140)\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "\n",
    "# -------- Outlier handling (NEW) --------\n",
    "PHYS_CLIP = True\n",
    "NDVI_BOUNDS = (-0.2, 1.0)\n",
    "EVI_BOUNDS  = (-0.2, 1.1)\n",
    "\n",
    "WINSORIZE_BY_ADM2_MONTH = True  # preserves seasonality structure\n",
    "WINSOR_P = 0.02                 # clip 2% tails within each ADM2×month\n",
    "\n",
    "MIN_CLEAR_MONTH = 0.15          # months below this clear fraction are nulled\n",
    "MIN_COUNT_MONTH = 2             # months with too few images are nulled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f782a-48e0-44cd-a866-b2cfc5f391e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) I/O helpers ----------\n",
    "NUMERIC_COLS_DEFAULT = [\n",
    "    \"mean_NDVI\",\"max_NDVI\",\"mean_EVI\",\"max_EVI\",\n",
    "    \"clear_frac_mean\",\"clear_frac_max\",\"count_images\"\n",
    "]\n",
    "\n",
    "def _read_one_csv(fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'date' column in {fp}\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"days_in_month\"] = df[\"date\"].dt.days_in_month\n",
    "\n",
    "    # Infer ADM2 from filename if absent\n",
    "    adm2_guess = Path(fp).name.split(\"_\")[0]\n",
    "    if \"ADM2_PCODE\" not in df.columns:\n",
    "        df[\"ADM2_PCODE\"] = adm2_guess\n",
    "    else:\n",
    "        df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].fillna(adm2_guess)\n",
    "\n",
    "    # Cast numerics\n",
    "    for col in NUMERIC_COLS_DEFAULT:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def load_monthly_files(input_dir: Path, max_workers: int = 12) -> pd.DataFrame:\n",
    "    files = glob.glob(str(input_dir / \"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No CSVs found in {input_dir}\")\n",
    "    frames = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_read_one_csv, fp): fp for fp in files}\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Loading CSVs\"):\n",
    "            try:\n",
    "                frames.append(f.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {futures[f]}: {e}\")\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # ---- De-duplicate per ADM2×date (averages numerics) to prevent double counting\n",
    "    keys = [\"ADM2_PCODE\",\"date\"]\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    agg = {c: \"mean\" for c in num_cols}\n",
    "    for k in keys:\n",
    "        agg[k] = \"first\"\n",
    "    df = (df.groupby(keys, as_index=False).agg(agg)\n",
    "            .sort_values(keys).reset_index(drop=True))\n",
    "    # Recompute calendar fields (robust)\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"days_in_month\"] = df[\"date\"].dt.days_in_month\n",
    "    return df\n",
    "\n",
    "# ---------- 2b) Cleaning utilities (NEW) ----------\n",
    "def _clip_physical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"mean_NDVI\" in out: out[\"mean_NDVI\"] = out[\"mean_NDVI\"].clip(*NDVI_BOUNDS)\n",
    "    if \"max_NDVI\"  in out: out[\"max_NDVI\"]  = out[\"max_NDVI\"].clip(*NDVI_BOUNDS)\n",
    "    if \"mean_EVI\"  in out: out[\"mean_EVI\"]  = out[\"mean_EVI\"].clip(*EVI_BOUNDS)\n",
    "    if \"max_EVI\"   in out: out[\"max_EVI\"]   = out[\"max_EVI\"].clip(*EVI_BOUNDS)\n",
    "    return out\n",
    "\n",
    "def _winsorize_by_groups(df: pd.DataFrame, cols, p=0.02, by=(\"ADM2_PCODE\",\"month\")) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if not cols: return out\n",
    "    # For each ADM2×month, clip to group quantiles\n",
    "    for _, idx in out.groupby(list(by)).groups.items():\n",
    "        g = out.loc[idx, cols]\n",
    "        if g.shape[0] < 3:  # too small to estimate tails\n",
    "            continue\n",
    "        lo = g.quantile(p)\n",
    "        hi = g.quantile(1 - p)\n",
    "        out.loc[idx, cols] = g.clip(lower=lo, upper=hi, axis=1)\n",
    "    return out\n",
    "\n",
    "def _gate_low_quality(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    has_clear = \"clear_frac_mean\" in out.columns\n",
    "    has_count = \"count_images\" in out.columns\n",
    "    if not (has_clear or has_count):\n",
    "        return out\n",
    "    mask_bad = pd.Series(False, index=out.index)\n",
    "    if has_clear:\n",
    "        mask_bad = mask_bad | (pd.to_numeric(out[\"clear_frac_mean\"], errors=\"coerce\") < MIN_CLEAR_MONTH)\n",
    "    if has_count:\n",
    "        mask_bad = mask_bad | (pd.to_numeric(out[\"count_images\"], errors=\"coerce\") < MIN_COUNT_MONTH)\n",
    "    for c in [\"mean_NDVI\",\"max_NDVI\",\"mean_EVI\",\"max_EVI\"]:\n",
    "        if c in out.columns:\n",
    "            out.loc[mask_bad, c] = np.nan\n",
    "    return out\n",
    "\n",
    "def clean_monthly_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if PHYS_CLIP:\n",
    "        out = _clip_physical(out)\n",
    "    out = _gate_low_quality(out)\n",
    "    if WINSORIZE_BY_ADM2_MONTH:\n",
    "        cols = [c for c in [\"mean_NDVI\",\"mean_EVI\"] if c in out.columns]\n",
    "        out = _winsorize_by_groups(out, cols=cols, p=WINSOR_P, by=(\"ADM2_PCODE\",\"month\"))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9478c-864a-4ab9-b7a1-9f360b01f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) Calendar & general helpers ----------\n",
    "def padded_limits(series: pd.Series, pad: float = 0.05):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return (0, 1)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if lo == hi:\n",
    "        span = abs(hi) if hi != 0 else 1.0\n",
    "        lo, hi = hi - 0.1*span, hi + 0.1*span\n",
    "    pad_span = (hi - lo) * pad\n",
    "    return (lo - pad_span, hi + pad_span)\n",
    "\n",
    "def slope_per_adm2(df: pd.DataFrame, value_col: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for adm2, sub in df[[\"ADM2_PCODE\",\"year\",value_col]].dropna().groupby(\"ADM2_PCODE\"):\n",
    "        x = sub[\"year\"].values.astype(float)\n",
    "        y = sub[value_col].values.astype(float)\n",
    "        slope = np.polyfit(x, y, 1)[0] if len(np.unique(x)) >= 2 else np.nan\n",
    "        rows.append({\"ADM2_PCODE\": adm2, f\"slope_{value_col}\": slope})\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531f71f-c7fe-42d9-b465-3b0dfb4075f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) AUC helpers (no groupby.apply warnings) ----------\n",
    "def annual_auc(df: pd.DataFrame, index_col: str) -> pd.DataFrame:\n",
    "    cols = [\"ADM2_PCODE\", \"year\", \"days_in_month\", index_col]\n",
    "    tmp = df.loc[:, cols].copy()\n",
    "    tmp[index_col] = pd.to_numeric(tmp[index_col], errors=\"coerce\")\n",
    "    tmp[\"days_in_month\"] = pd.to_numeric(tmp[\"days_in_month\"], errors=\"coerce\")\n",
    "    tmp[\"w\"] = tmp[index_col] * tmp[\"days_in_month\"]\n",
    "    out = (tmp.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[\"w\"].sum()\n",
    "              .rename(columns={\"w\": f\"AUC_{index_col}\"}))\n",
    "    return out\n",
    "\n",
    "def annual_auc_clearweighted(df: pd.DataFrame, index_col: str) -> pd.DataFrame:\n",
    "    if \"clear_frac_mean\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"ADM2_PCODE\",\"year\",f\"AUCcw_{index_col}\"])\n",
    "    cols = [\"ADM2_PCODE\",\"year\",\"days_in_month\",index_col,\"clear_frac_mean\"]\n",
    "    tmp = df.loc[:, cols].copy()\n",
    "    tmp[index_col] = pd.to_numeric(tmp[index_col], errors=\"coerce\")\n",
    "    tmp[\"days_in_month\"] = pd.to_numeric(tmp[\"days_in_month\"], errors=\"coerce\")\n",
    "    tmp[\"clear_frac_mean\"] = pd.to_numeric(tmp[\"clear_frac_mean\"], errors=\"coerce\").fillna(0).clip(0,1)\n",
    "    tmp[\"w\"] = tmp[index_col] * tmp[\"days_in_month\"] * tmp[\"clear_frac_mean\"]\n",
    "    out = (tmp.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[\"w\"].sum()\n",
    "              .rename(columns={\"w\": f\"AUCcw_{index_col}\"}))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345002ab-0081-4a76-adbf-1b89a387078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) Estimated harvest windows ----------\n",
    "def _months_in_span(start_m: int, end_m: int) -> set:\n",
    "    if pd.isna(start_m) or pd.isna(end_m): return set()\n",
    "    s, e = int(start_m), int(end_m)\n",
    "    if s <= e: return set(range(s, e+1))\n",
    "    return set(list(range(s, 13)) + list(range(1, e+1)))\n",
    "\n",
    "def _cyclic_roll(arr, shift):\n",
    "    shift = shift % len(arr)\n",
    "    return np.concatenate([arr[shift:], arr[:shift]]) if shift else arr.copy()\n",
    "\n",
    "def _smooth_cyclic(vals, window=3):\n",
    "    assert len(vals) == 12\n",
    "    padded = np.r_[vals[-(window//2):], vals, vals[:(window//2)]]\n",
    "    return np.convolve(padded, np.ones(window)/window, mode='valid')\n",
    "\n",
    "def _interp_cyclic(months, vals):\n",
    "    m = np.asarray(months, dtype=float)\n",
    "    v = np.asarray(vals, dtype=float)\n",
    "    isnan = np.isnan(v)\n",
    "    if isnan.all(): return v\n",
    "    m_ext = np.r_[0, m, 13]\n",
    "    v_ext = np.r_[v[11], v, v[0]]\n",
    "    good = ~np.isnan(v_ext)\n",
    "    v_interp = np.interp(m_ext, m_ext[good], v_ext[good])\n",
    "    return v_interp[1:-1]\n",
    "\n",
    "def _build_recent_climatology(df_adm2, value_col):\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    yrs = sorted(df_adm2[\"year\"].unique().tolist())\n",
    "    yrs_recent = [y for y in ANALYSIS_YEARS if y in yrs]\n",
    "    use_years = yrs_recent if len(yrs_recent) >= 2 else yrs\n",
    "    piv = (df_adm2[df_adm2[\"year\"].isin(use_years)]\n",
    "           .pivot_table(index=\"month\", values=value_col, aggfunc=\"mean\"))\n",
    "    return months, piv.reindex(months).values.ravel()\n",
    "\n",
    "def _build_recent_clear(df_adm2):\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    if \"clear_frac_mean\" not in df_adm2.columns:\n",
    "        return months, np.full(12, np.nan)\n",
    "    piv = (df_adm2.pivot_table(index=\"month\", values=\"clear_frac_mean\", aggfunc=\"mean\"))\n",
    "    return months, piv.reindex(months).values.ravel()\n",
    "\n",
    "def _estimate_window_from_series(months, vals, clear=None, min_clear=0.20):\n",
    "    months = np.asarray(months, dtype=int)\n",
    "    x = np.asarray(vals, dtype=float)\n",
    "    if clear is not None:\n",
    "        c = np.asarray(clear, dtype=float)\n",
    "        x = np.where((~np.isnan(c)) & (c < min_clear), np.nan, x)\n",
    "    x_filled = _interp_cyclic(months, x)\n",
    "    x_sm = _smooth_cyclic(x_filled, 3)\n",
    "\n",
    "    peak_idx = int(np.nanargmax(x_sm))\n",
    "    peak_month = int(months[peak_idx])\n",
    "\n",
    "    rolled = _cyclic_roll(x_sm, peak_idx)\n",
    "    search_slice = rolled[1:7]\n",
    "    trough_rel = int(np.nanargmin(search_slice)) + 1\n",
    "    trough_idx = (peak_idx + trough_rel) % 12\n",
    "    trough_month = int(months[trough_idx])\n",
    "\n",
    "    peak = x_sm[peak_idx]; trough = x_sm[trough_idx]\n",
    "    R = float(peak - trough)\n",
    "    T1 = peak - 0.20 * R; T2 = peak - 0.60 * R\n",
    "\n",
    "    start_idx = end_idx = None\n",
    "    for k in range(1, 7):\n",
    "        idx = (peak_idx + k) % 12\n",
    "        if start_idx is None and x_sm[idx] <= T1:\n",
    "            start_idx = idx\n",
    "        if start_idx is not None and x_sm[idx] <= T2:\n",
    "            end_idx = idx; break\n",
    "\n",
    "    if start_idx is None or end_idx is None:\n",
    "        diffs = np.r_[np.diff(x_sm), x_sm[0]-x_sm[-1]]\n",
    "        idxs = [(peak_idx + k) % 12 for k in range(1, 7)]\n",
    "        neg_slopes = diffs[idxs]\n",
    "        center_rel = int(np.nanargmin(neg_slopes)) + 1\n",
    "        center_idx = (peak_idx + center_rel) % 12\n",
    "        start_idx = start_idx or center_idx\n",
    "        end_idx   = end_idx   or ((center_idx + 1) % 12)\n",
    "        method = \"steepest_decline_fallback\"\n",
    "    else:\n",
    "        method = \"percent_of_range\"\n",
    "\n",
    "    start_month = int(months[start_idx]); end_month = int(months[end_idx])\n",
    "\n",
    "    seq = []\n",
    "    i = start_idx\n",
    "    while True:\n",
    "        j = (i + 1) % 12\n",
    "        seq.append(x_sm[j] - x_sm[i])\n",
    "        i = j\n",
    "        if i == end_idx: break\n",
    "    monotone = float(np.mean(np.array(seq) < 0)) if seq else 0.0\n",
    "    conf = float(np.clip((R / 0.35) * (0.5 + 0.5 * monotone), 0, 1))\n",
    "\n",
    "    return dict(\n",
    "        start_month=start_month, end_month=end_month,\n",
    "        peak_month=peak_month, trough_month=trough_month,\n",
    "        range_R=round(R,3), confidence=round(conf,3), method=method\n",
    "    )\n",
    "\n",
    "def estimate_harvest_windows(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns one row per ADM2 with NDVI/EVI+consensus windows; also writes CSV.\"\"\"\n",
    "    recs = []\n",
    "    for adm2 in tqdm(sorted(data[\"ADM2_PCODE\"].unique()), desc=\"Estimating harvest windows\"):\n",
    "        sub = data.loc[data[\"ADM2_PCODE\"]==adm2,\n",
    "                       [\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\",\"clear_frac_mean\"]].copy()\n",
    "        m_ndvi, s_ndvi = _build_recent_climatology(sub, \"mean_NDVI\")\n",
    "        m_evi,  s_evi  = _build_recent_climatology(sub, \"mean_EVI\")\n",
    "        m_clr,  s_clr  = _build_recent_clear(sub)\n",
    "\n",
    "        nd = _estimate_window_from_series(m_ndvi, s_ndvi, s_clr)\n",
    "        ev = _estimate_window_from_series(m_evi,  s_evi,  s_clr)\n",
    "\n",
    "        def _span_to_set(a,b):\n",
    "            return _months_in_span(a,b)\n",
    "\n",
    "        nd_set = _span_to_set(nd[\"start_month\"], nd[\"end_month\"])\n",
    "        ev_set = _span_to_set(ev[\"start_month\"], ev[\"end_month\"])\n",
    "        uni    = sorted(list(nd_set | ev_set))\n",
    "        if not uni:\n",
    "            cons_s, cons_e, cons_m = nd[\"start_month\"], nd[\"end_month\"], \"NDVI_only\"\n",
    "        else:\n",
    "            cons_s, cons_e, cons_m = uni[0], uni[-1], \"union(NDVI,EVI)\"\n",
    "\n",
    "        recs.append({\n",
    "            \"ADM2_PCODE\":               adm2,\n",
    "            \"ndvi_start_month\":         nd[\"start_month\"],\n",
    "            \"ndvi_end_month\":           nd[\"end_month\"],\n",
    "            \"ndvi_peak_month\":          nd[\"peak_month\"],\n",
    "            \"ndvi_trough_month\":        nd[\"trough_month\"],\n",
    "            \"ndvi_range\":               nd[\"range_R\"],\n",
    "            \"ndvi_confidence\":          nd[\"confidence\"],\n",
    "            \"ndvi_method\":              nd[\"method\"],\n",
    "            \"evi_start_month\":          ev[\"start_month\"],\n",
    "            \"evi_end_month\":            ev[\"end_month\"],\n",
    "            \"evi_peak_month\":           ev[\"peak_month\"],\n",
    "            \"evi_trough_month\":         ev[\"trough_month\"],\n",
    "            \"evi_range\":                ev[\"range_R\"],\n",
    "            \"evi_confidence\":           ev[\"confidence\"],\n",
    "            \"evi_method\":               ev[\"method\"],\n",
    "            \"consensus_start_month\":    cons_s,\n",
    "            \"consensus_end_month\":      cons_e,\n",
    "            \"consensus_method\":         cons_m\n",
    "        })\n",
    "    hw = pd.DataFrame.from_records(recs)\n",
    "    hw.to_csv(HARVEST_WIN_CSV, index=False)\n",
    "    print(\"Saved harvest window estimates to:\", HARVEST_WIN_CSV)\n",
    "    return hw\n",
    "\n",
    "# Vectorized harvest means over estimated windows\n",
    "def harvest_means_estimated(df: pd.DataFrame, index_col: str,\n",
    "                            hw_df: pd.DataFrame, which: str = \"ndvi\") -> pd.DataFrame:\n",
    "    if which not in {\"ndvi\",\"evi\",\"consensus\"}:\n",
    "        raise ValueError(\"which must be 'ndvi','evi', or 'consensus'\")\n",
    "    if which == \"ndvi\":\n",
    "        cols = (\"ndvi_start_month\",\"ndvi_end_month\")\n",
    "    elif which == \"evi\":\n",
    "        cols = (\"evi_start_month\",\"evi_end_month\")\n",
    "    else:\n",
    "        cols = (\"consensus_start_month\",\"consensus_end_month\")\n",
    "\n",
    "    # Expand windows → long table [ADM2_PCODE, month] allowed\n",
    "    rows = []\n",
    "    for r in hw_df.itertuples(index=False):\n",
    "        mset = _months_in_span(getattr(r, cols[0]), getattr(r, cols[1]))\n",
    "        rows += [(r.ADM2_PCODE, m) for m in sorted(mset)]\n",
    "    allow = pd.DataFrame(rows, columns=[\"ADM2_PCODE\",\"month\"])\n",
    "    if allow.empty:\n",
    "        return pd.DataFrame(columns=[\"ADM2_PCODE\",\"year\",f\"harv_{index_col}\"])\n",
    "\n",
    "    # Join and average\n",
    "    sub = df[[\"ADM2_PCODE\",\"year\",\"month\",index_col]].copy()\n",
    "    sub[index_col] = pd.to_numeric(sub[index_col], errors=\"coerce\")\n",
    "    merged = sub.merge(allow, on=[\"ADM2_PCODE\",\"month\"], how=\"inner\")\n",
    "    out = (merged.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[index_col]\n",
    "                 .mean().rename(columns={index_col: f\"harv_{index_col}\"}))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728fbc4-acf4-4737-9798-f4ce4b407813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6) Plots ----------\n",
    "def plot_metric_by_adm2(df: pd.DataFrame, value_col: str, title: str, out_png: Path, ylim=None):\n",
    "    plt.figure()\n",
    "    for adm2, sub in df.sort_values([\"ADM2_PCODE\",\"year\"]).groupby(\"ADM2_PCODE\"):\n",
    "        plt.plot(sub[\"year\"], sub[value_col], label=adm2)\n",
    "    plt.title(title); plt.xlabel(\"Year\"); plt.ylabel(value_col)\n",
    "    if ylim is not None: plt.ylim(ylim)\n",
    "    plt.legend(ncols=2, fontsize=8)\n",
    "    plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "\n",
    "def plot_per_adm2(df: pd.DataFrame, value_col: str, out_dir: Path, ylim=None, title_prefix=\"\"):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for adm2, sub in tqdm(df.groupby(\"ADM2_PCODE\"), total=df[\"ADM2_PCODE\"].nunique(), desc=f\"Plots: {value_col}\"):\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"year\"], sub[value_col])\n",
    "        plt.title(f\"{title_prefix}{adm2}\")\n",
    "        plt.xlabel(\"Year\"); plt.ylabel(value_col)\n",
    "        if ylim is not None: plt.ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / f\"{adm2}_{value_col}.png\")\n",
    "        plt.close()\n",
    "\n",
    "MONTH_LABELS = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "def _shade_window_spans(start_m, end_m):\n",
    "    if pd.isna(start_m) or pd.isna(end_m): return []\n",
    "    s, e = int(start_m), int(end_m)\n",
    "    if 1 <= s <= 12 and 1 <= e <= 12:\n",
    "        if s <= e: return [(s - 0.5, e + 0.5)]\n",
    "        return [(0.5, e + 0.5), (s - 0.5, 12.5)]\n",
    "    return []\n",
    "\n",
    "def seasonal_overlay(adm2_df: pd.DataFrame, value_col: str, start_m: int, end_m: int, out_path: Path):\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    piv = (adm2_df.pivot_table(index=\"month\", columns=\"year\", values=value_col, aggfunc=\"mean\")\n",
    "           .reindex(index=months))\n",
    "    plt.figure()\n",
    "    for y in sorted([c for c in piv.columns if pd.notna(c)]):\n",
    "        plt.plot(months, piv[y].values, label=str(int(y)))\n",
    "    base_cols = [y for y in BASELINE_YEARS if y in piv.columns]\n",
    "    if base_cols:\n",
    "        plt.plot(months, piv[base_cols].mean(axis=1).values, linewidth=3, label=\"Baseline (2019–2021)\")\n",
    "    for x0, x1 in _shade_window_spans(start_m, end_m):\n",
    "        plt.axvspan(x0, x1, alpha=0.12)\n",
    "    plt.xticks(months, MONTH_LABELS); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Month\"); plt.ylabel(value_col)\n",
    "    adm2 = str(adm2_df[\"ADM2_PCODE\"].iloc[0])\n",
    "    title_h = (f\" (est. harvest {MONTH_LABELS[int(start_m)-1]}–{MONTH_LABELS[int(end_m)-1]})\"\n",
    "               if not (pd.isna(start_m) or pd.isna(end_m)) else \" (no estimate)\")\n",
    "    metric = \"NDVI\" if \"NDVI\" in value_col else \"EVI\"\n",
    "    plt.title(f\"{adm2} — Seasonal overlay ({metric}){title_h}\")\n",
    "    plt.legend(ncols=2, fontsize=8)\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def _eom(y, m): return datetime(y, m, calendar.monthrange(y, m)[1])\n",
    "\n",
    "def _year_shades(y, start_m, end_m):\n",
    "    if pd.isna(start_m) or pd.isna(end_m): return []\n",
    "    s, e = int(start_m), int(end_m)\n",
    "    spans = []\n",
    "    if s <= e:\n",
    "        spans.append((datetime(y, s, 1), _eom(y, e)))\n",
    "    else:\n",
    "        spans.append((datetime(y, 1, 1), _eom(y, e)))\n",
    "        spans.append((datetime(y, s, 1), datetime(y, 12, 31)))\n",
    "    return spans\n",
    "\n",
    "def timeseries_with_shading(adm2_df: pd.DataFrame, value_col: str, start_m: int, end_m: int, out_path: Path):\n",
    "    dfp = adm2_df.sort_values(\"date\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(dfp[\"date\"], dfp[value_col], marker=\"o\", linewidth=1.5, label=value_col)\n",
    "    for y in sorted(dfp[\"year\"].unique()):\n",
    "        for d0, d1 in _year_shades(y, start_m, end_m):\n",
    "            left  = max(d0, dfp[\"date\"].min().to_pydatetime())\n",
    "            right = min(d1, dfp[\"date\"].max().to_pydatetime())\n",
    "            if left <= right:\n",
    "                ax.axvspan(left, right, alpha=0.12)\n",
    "    ax.set_ylim(0,1); ax.set_ylabel(value_col); ax.set_xlabel(\"Date\")\n",
    "    adm2 = str(dfp[\"ADM2_PCODE\"].iloc[0])\n",
    "    title_h = (f\" (est. harvest {MONTH_LABELS[int(start_m)-1]}–{MONTH_LABELS[int(end_m)-1]})\"\n",
    "               if not (pd.isna(start_m) or pd.isna(end_m)) else \" (no estimate)\")\n",
    "    ax.set_title(f\"{adm2} — Monthly time series{title_h}\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.2); ax.legend(loc=\"upper right\", fontsize=8)\n",
    "    fig.tight_layout(); fig.savefig(out_path); plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f53b6d-90b6-408f-a5ba-81bd7eabd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 7) Load monthly, CLEAN, & estimate harvest windows ----------\n",
    "data = load_monthly_files(INPUT_DIR)\n",
    "print(\"Rows (raw):\", len(data), \" | ADM2s:\", data[\"ADM2_PCODE\"].nunique())\n",
    "\n",
    "# *** CLEAN OUTLIERS HERE ***\n",
    "data = clean_monthly_outliers(data)\n",
    "\n",
    "harv_windows = estimate_harvest_windows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f7659-24b0-4d22-ad75-d39e34c0230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 8) Build annual metrics (AUCs & harvest-window means) ----------\n",
    "auc_ndvi    = annual_auc(data, \"mean_NDVI\")\n",
    "auc_evi     = annual_auc(data, \"mean_EVI\")\n",
    "auc_ndvi_cw = annual_auc_clearweighted(data, \"mean_NDVI\")\n",
    "auc_evi_cw  = annual_auc_clearweighted(data, \"mean_EVI\")\n",
    "\n",
    "if USE_CONSENSUS_FOR_BOTH:\n",
    "    harv_ndvi = harvest_means_estimated(data, \"mean_NDVI\", harv_windows, which=\"consensus\")\n",
    "    harv_evi  = harvest_means_estimated(data, \"mean_EVI\",  harv_windows, which=\"consensus\")\n",
    "else:\n",
    "    harv_ndvi = harvest_means_estimated(data, \"mean_NDVI\", harv_windows, which=\"ndvi\")\n",
    "    harv_evi  = harvest_means_estimated(data, \"mean_EVI\",  harv_windows, which=\"evi\")\n",
    "\n",
    "metrics = (auc_ndvi.merge(auc_evi,     on=[\"ADM2_PCODE\",\"year\"], how=\"outer\")\n",
    "                    .merge(auc_ndvi_cw, on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(auc_evi_cw,  on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(harv_ndvi,   on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(harv_evi,    on=[\"ADM2_PCODE\",\"year\"], how=\"left\"))\n",
    "\n",
    "# Ensure uniqueness per ADM2×year (safety against accidental duplicates)\n",
    "if metrics.duplicated([\"ADM2_PCODE\",\"year\"]).any():\n",
    "    num_cols = metrics.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    metrics = (metrics.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[num_cols].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07892b67-6117-4f0f-a2c9-367020b1d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 9) Baseline (2019–2021) & anomalies ----------\n",
    "base = (metrics[metrics[\"year\"].isin(BASELINE_YEARS)]\n",
    "        .groupby(\"ADM2_PCODE\")\n",
    "        .agg({\n",
    "            \"AUC_mean_NDVI\":\"mean\",\"AUC_mean_EVI\":\"mean\",\n",
    "            \"harv_mean_NDVI\":\"mean\",\"harv_mean_EVI\":\"mean\",\n",
    "            \"AUCcw_mean_NDVI\":\"mean\",\"AUCcw_mean_EVI\":\"mean\",\n",
    "        }).rename(columns={\n",
    "            \"AUC_mean_NDVI\":\"base_AUC_NDVI\",\n",
    "            \"AUC_mean_EVI\":\"base_AUC_EVI\",\n",
    "            \"harv_mean_NDVI\":\"base_harv_NDVI\",\n",
    "            \"harv_mean_EVI\":\"base_harv_EVI\",\n",
    "            \"AUCcw_mean_NDVI\":\"base_AUCcw_NDVI\",\n",
    "            \"AUCcw_mean_EVI\":\"base_AUCcw_EVI\",\n",
    "        }).reset_index())\n",
    "\n",
    "metrics = metrics.merge(base, on=\"ADM2_PCODE\", how=\"left\")\n",
    "\n",
    "for col, bcol in [\n",
    "    (\"AUC_mean_NDVI\",\"base_AUC_NDVI\"),\n",
    "    (\"AUC_mean_EVI\",\"base_AUC_EVI\"),\n",
    "    (\"harv_mean_NDVI\",\"base_harv_NDVI\"),\n",
    "    (\"harv_mean_EVI\",\"base_harv_EVI\"),\n",
    "    (\"AUCcw_mean_NDVI\",\"base_AUCcw_NDVI\"),\n",
    "    (\"AUCcw_mean_EVI\",\"base_AUCcw_EVI\"),\n",
    "]:\n",
    "    if col in metrics.columns:\n",
    "        metrics[f\"{col}_anom\"] = metrics[col] - metrics[bcol]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            metrics[f\"{col}_anom_pct\"] = np.where(\n",
    "                (metrics[bcol].notna()) & (metrics[bcol].abs() > 0),\n",
    "                (metrics[f\"{col}_anom\"] / metrics[bcol]) * 100.0,\n",
    "                np.nan\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb3bd4-fa4b-4626-9c2b-6263968cccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 10) Trends & QA ----------\n",
    "trend_auc_ndvi = slope_per_adm2(metrics, \"AUC_mean_NDVI\")\n",
    "trend_auc_evi  = slope_per_adm2(metrics, \"AUC_mean_EVI\")\n",
    "trend_hndvi    = slope_per_adm2(metrics, \"harv_mean_NDVI\")\n",
    "trend_hevi     = slope_per_adm2(metrics, \"harv_mean_EVI\")\n",
    "trend = (trend_auc_ndvi.merge(trend_auc_evi, on=\"ADM2_PCODE\", how=\"outer\")\n",
    "                        .merge(trend_hndvi,   on=\"ADM2_PCODE\", how=\"outer\")\n",
    "                        .merge(trend_hevi,    on=\"ADM2_PCODE\", how=\"outer\"))\n",
    "\n",
    "# QA: average clear fraction for union of NDVI/EVI harvest months in analysis years\n",
    "if \"clear_frac_mean\" in data.columns:\n",
    "    hw_map_nd = {r.ADM2_PCODE: _months_in_span(r.ndvi_start_month, r.ndvi_end_month)\n",
    "                 for r in harv_windows.itertuples(index=False)}\n",
    "    hw_map_ev = {r.ADM2_PCODE: _months_in_span(r.evi_start_month,  r.evi_end_month)\n",
    "                 for r in harv_windows.itertuples(index=False)}\n",
    "    union_map = {k: (hw_map_nd.get(k,set()) | hw_map_ev.get(k,set()))\n",
    "                 for k in set(hw_map_nd) | set(hw_map_ev)}\n",
    "    dqa = data[data[\"year\"].isin(ANALYSIS_YEARS)].copy()\n",
    "    dqa[\"__in_hw_union__\"] = dqa.apply(lambda r: r[\"month\"] in union_map.get(r[\"ADM2_PCODE\"], set()), axis=1)\n",
    "    qa_summary = (dqa[dqa[\"__in_hw_union__\"]]\n",
    "                  .groupby(\"ADM2_PCODE\")[\"clear_frac_mean\"]\n",
    "                  .mean().reset_index(name=\"avg_clear_frac_harv_2022_2025\"))\n",
    "else:\n",
    "    qa_summary = pd.DataFrame({\"ADM2_PCODE\": metrics[\"ADM2_PCODE\"].unique(),\n",
    "                               \"avg_clear_frac_harv_2022_2025\": np.nan})\n",
    "\n",
    "# Final summary per ADM2 (averages for 2022–2025)\n",
    "summary = (base.merge(\n",
    "    metrics[metrics[\"year\"].isin(ANALYSIS_YEARS)]\n",
    "    .groupby(\"ADM2_PCODE\")\n",
    "    .agg({\n",
    "        \"AUC_mean_NDVI_anom\":\"mean\",\n",
    "        \"AUC_mean_EVI_anom\":\"mean\",\n",
    "        \"AUC_mean_NDVI_anom_pct\":\"mean\",\n",
    "        \"AUC_mean_EVI_anom_pct\":\"mean\",\n",
    "        \"AUCcw_mean_NDVI_anom\":\"mean\",\n",
    "        \"AUCcw_mean_EVI_anom\":\"mean\",\n",
    "        \"AUCcw_mean_NDVI_anom_pct\":\"mean\",\n",
    "        \"AUCcw_mean_EVI_anom_pct\":\"mean\",\n",
    "        \"harv_mean_NDVI_anom\":\"mean\",\n",
    "        \"harv_mean_EVI_anom\":\"mean\",\n",
    "    }).rename(columns={\n",
    "        \"AUC_mean_NDVI_anom\":\"avg_AUC_NDVI_anom_2022_2025\",\n",
    "        \"AUC_mean_EVI_anom\":\"avg_AUC_EVI_anom_2022_2025\",\n",
    "        \"AUC_mean_NDVI_anom_pct\":\"avg_AUC_NDVI_anom_pct_2022_2025\",\n",
    "        \"AUC_mean_EVI_anom_pct\":\"avg_AUC_EVI_anom_pct_2022_2025\",\n",
    "        \"AUCcw_mean_NDVI_anom\":\"avg_AUCcw_NDVI_anom_2022_2025\",\n",
    "        \"AUCcw_mean_EVI_anom\":\"avg_AUCcw_EVI_anom_2022_2025\",\n",
    "        \"AUCcw_mean_NDVI_anom_pct\":\"avg_AUCcw_NDVI_anom_pct_2022_2025\",\n",
    "        \"AUCcw_mean_EVI_anom_pct\":\"avg_AUCcw_EVI_anom_pct_2022_2025\",\n",
    "        \"harv_mean_NDVI_anom\":\"avg_harv_NDVI_anom_2022_2025\",\n",
    "        \"harv_mean_EVI_anom\":\"avg_harv_EVI_anom_2022_2025\",\n",
    "    }), on=\"ADM2_PCODE\", how=\"left\")\n",
    "    .merge(trend, on=\"ADM2_PCODE\", how=\"left\")\n",
    "    .merge(qa_summary, on=\"ADM2_PCODE\", how=\"left\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75187b3-9fcf-4fd1-83df-82aaed9b772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 11) Save CSVs ----------\n",
    "summary_path = OUTPUT_DIR / \"panel_summary_by_ADM2.csv\"\n",
    "metrics_path = OUTPUT_DIR / \"panel_yearly_metrics_long.csv\"\n",
    "qa_path      = OUTPUT_DIR / \"panel_QA.csv\"\n",
    "\n",
    "summary.to_csv(summary_path, index=False)\n",
    "metrics.to_csv(metrics_path, index=False)\n",
    "qa_summary.to_csv(qa_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", summary_path)\n",
    "print(\"  \", metrics_path)\n",
    "print(\"  \", qa_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f79cc5-f70f-4cc8-a430-79a942f05744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 12) Aggregate & per-ADM2 plots ----------\n",
    "auc_ndvi_ylim = padded_limits(auc_ndvi[\"AUC_mean_NDVI\"], pad=0.08)\n",
    "auc_evi_ylim  = padded_limits(auc_evi[\"AUC_mean_EVI\"],   pad=0.08)\n",
    "\n",
    "plot_metric_by_adm2(auc_ndvi, \"AUC_mean_NDVI\",\n",
    "                    \"Annual AUC (mean NDVI) by ADM2\",\n",
    "                    PLOT_DIR_AUC_NDVI / \"overview_auc_ndvi.png\",\n",
    "                    ylim=auc_ndvi_ylim)\n",
    "plot_metric_by_adm2(auc_evi,  \"AUC_mean_EVI\",\n",
    "                    \"Annual AUC (mean EVI) by ADM2\",\n",
    "                    PLOT_DIR_AUC_EVI / \"overview_auc_evi.png\",\n",
    "                    ylim=auc_evi_ylim)\n",
    "plot_metric_by_adm2(harv_ndvi, \"harv_mean_NDVI\",\n",
    "                    \"Harvest-window Mean NDVI (estimated) by ADM2\",\n",
    "                    PLOT_DIR_HARV_MEAN_NDVI / \"overview_harvest_mean_ndvi.png\",\n",
    "                    ylim=(0,1))\n",
    "plot_metric_by_adm2(harv_evi,  \"harv_mean_EVI\",\n",
    "                    \"Harvest-window Mean EVI (estimated) by ADM2\",\n",
    "                    PLOT_DIR_HARV_MEAN_EVI / \"overview_harvest_mean_evi.png\",\n",
    "                    ylim=(0,1))\n",
    "\n",
    "plot_per_adm2(auc_ndvi, \"AUC_mean_NDVI\", out_dir=PLOT_DIR_AUC_NDVI, ylim=auc_ndvi_ylim, title_prefix=\"AUC (NDVI) — \")\n",
    "plot_per_adm2(auc_evi,  \"AUC_mean_EVI\",  out_dir=PLOT_DIR_AUC_EVI,  ylim=auc_evi_ylim,  title_prefix=\"AUC (EVI) — \")\n",
    "plot_per_adm2(harv_ndvi,\"harv_mean_NDVI\",out_dir=PLOT_DIR_HARV_MEAN_NDVI, ylim=(0,1), title_prefix=\"Harvest mean NDVI (est.) — \")\n",
    "plot_per_adm2(harv_evi, \"harv_mean_EVI\", out_dir=PLOT_DIR_HARV_MEAN_EVI,  ylim=(0,1), title_prefix=\"Harvest mean EVI (est.) — \")\n",
    "\n",
    "print(\"Saved aggregate/per-ADM2 plots under:\", PLOTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0958f8-2941-4823-9e26-81170800552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 13) Rankings & compact report ----------\n",
    "# Helpers\n",
    "def zscore_winsor(s: pd.Series, p: float = 0.05) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.notna().sum() < 2: return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    lo, hi = s.quantile([p, 1-p]); s2 = s.clip(lo, hi)\n",
    "    std = s2.std(ddof=0)\n",
    "    return pd.Series(np.zeros(len(s)), index=s.index) if std == 0 or np.isnan(std) else (s2 - s2.mean())/std\n",
    "\n",
    "def pct_rank(s: pd.Series) -> pd.Series:\n",
    "    return 100 * s.rank(pct=True, method=\"average\")\n",
    "\n",
    "# Fill any missing required columns\n",
    "for col in [\n",
    "    \"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\",\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "    \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "    \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\n",
    "    \"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\",\n",
    "    \"avg_clear_frac_harv_2022_2025\"\n",
    "]:\n",
    "    if col not in summary.columns: summary[col] = np.nan\n",
    "\n",
    "W_LEVEL = {\"base_AUC_NDVI\":1.0,\"base_AUC_EVI\":1.0,\"base_harv_NDVI\":0.5,\"base_harv_EVI\":0.5}\n",
    "W_MOM   = {\"avg_AUC_NDVI_anom_2022_2025\":1.0,\"avg_AUC_EVI_anom_2022_2025\":1.0,\n",
    "           \"avg_harv_NDVI_anom_2022_2025\":0.5,\"avg_harv_EVI_anom_2022_2025\":0.5,\n",
    "           \"slope_AUC_mean_NDVI\":0.75,\"slope_AUC_mean_EVI\":0.75,\n",
    "           \"slope_harv_mean_NDVI\":0.5,\"slope_harv_mean_EVI\":0.5}\n",
    "ALPHA_LEVEL = 0.5; ALPHA_MOM = 0.5\n",
    "BETA_QA = 1.0\n",
    "qa_w = lambda cf: (0.5 + 0.5 * pd.to_numeric(cf, errors=\"coerce\").fillna(1.0))\n",
    "\n",
    "summary[\"LevelScore\"]    = np.sum([w * zscore_winsor(summary[c]) for c, w in W_LEVEL.items()], axis=0)\n",
    "summary[\"MomentumScore\"] = np.sum([w * zscore_winsor(summary[c]) for c, w in W_MOM.items()], axis=0)\n",
    "summary[\"CombinedScore\"] = ALPHA_LEVEL*summary[\"LevelScore\"] + ALPHA_MOM*summary[\"MomentumScore\"]\n",
    "summary[\"ClearWeight\"]   = qa_w(summary[\"avg_clear_frac_harv_2022_2025\"])\n",
    "summary[\"WeightedCombined\"] = summary[\"CombinedScore\"] * (summary[\"ClearWeight\"] ** BETA_QA)\n",
    "\n",
    "summary[\"Rank_Level\"]    = summary[\"LevelScore\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_Momentum\"] = summary[\"MomentumScore\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_Combined\"] = summary[\"CombinedScore\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_WeightedCombined\"] = summary[\"WeightedCombined\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Pct_WeightedCombined\"]  = pct_rank(summary[\"WeightedCombined\"])\n",
    "\n",
    "leaderboard = summary.sort_values(\"WeightedCombined\", ascending=False).reset_index(drop=True)\n",
    "out_overall = OUTPUT_DIR / \"adm2_rank_overall.csv\"\n",
    "leaderboard_cols = [\n",
    "    \"ADM2_PCODE\",\"LevelScore\",\"Rank_Level\",\"MomentumScore\",\"Rank_Momentum\",\n",
    "    \"CombinedScore\",\"Rank_Combined\",\"ClearWeight\",\"WeightedCombined\",\"Rank_WeightedCombined\",\"Pct_WeightedCombined\",\n",
    "    \"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\",\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "    \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "    \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\",\n",
    "    \"avg_clear_frac_harv_2022_2025\"\n",
    "]\n",
    "leaderboard[leaderboard_cols].to_csv(out_overall, index=False)\n",
    "print(\"Saved overall ranking to:\", out_overall)\n",
    "\n",
    "# Per-year ranking sheet\n",
    "def per_year_score(dfy):\n",
    "    return (1.0*zscore_winsor(dfy[\"AUC_mean_NDVI\"]) + 1.0*zscore_winsor(dfy[\"AUC_mean_EVI\"]) +\n",
    "            0.5*zscore_winsor(dfy[\"harv_mean_NDVI\"]) + 0.5*zscore_winsor(dfy[\"harv_mean_EVI\"]))\n",
    "rows = []\n",
    "for yr, dfy in metrics.groupby(\"year\"):\n",
    "    s = per_year_score(dfy)\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"ADM2_PCODE\": dfy[\"ADM2_PCODE\"].values,\n",
    "        \"year\": yr,\n",
    "        \"YearScore\": s.values,\n",
    "        \"YearRank\": s.rank(ascending=False, method=\"min\").values,\n",
    "        \"YearPct\": pct_rank(s).values\n",
    "    }))\n",
    "yearly_rank = pd.concat(rows, ignore_index=True).sort_values([\"year\",\"YearRank\"])\n",
    "yearly_rank.to_csv(OUTPUT_DIR / \"adm2_rank_by_year.csv\", index=False)\n",
    "\n",
    "# Compact workbook + category tables (all ADM2s)\n",
    "def _cv(series: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    return np.nan if len(s) < 3 or s.mean()==0 else float(s.std(ddof=0)/s.mean())\n",
    "\n",
    "cv_all = (metrics.groupby(\"ADM2_PCODE\")\n",
    "          .agg(cv_auc_ndvi=(\"AUC_mean_NDVI\", _cv),\n",
    "               cv_auc_evi =(\"AUC_mean_EVI\",  _cv))\n",
    "          .reset_index())\n",
    "cv_all[\"cv_auc_mean\"] = cv_all[[\"cv_auc_ndvi\",\"cv_auc_evi\"]].mean(axis=1, skipna=True)\n",
    "\n",
    "overall_leaders_full = (summary.assign(OverallRank=lambda d: d[\"WeightedCombined\"].rank(ascending=False, method=\"min\"))\n",
    "                        .sort_values(\"WeightedCombined\", ascending=False)\n",
    "                        .loc[:, [\"ADM2_PCODE\",\"OverallRank\",\"WeightedCombined\",\"ClearWeight\",\"CombinedScore\",\"LevelScore\",\"MomentumScore\"]])\n",
    "top_baseline_full = (summary.assign(BaselineRank=lambda d: d[\"LevelScore\"].rank(ascending=False, method=\"min\"))\n",
    "                     .sort_values(\"LevelScore\", ascending=False)\n",
    "                     .loc[:, [\"ADM2_PCODE\",\"BaselineRank\",\"LevelScore\",\"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\"]])\n",
    "top_improvers_full = (summary.assign(ImproverRank=lambda d: d[\"MomentumScore\"].rank(ascending=False, method=\"min\"))\n",
    "                      .sort_values(\"MomentumScore\", ascending=False)\n",
    "                      .loc[:, [\"ADM2_PCODE\",\"ImproverRank\",\"MomentumScore\",\n",
    "                               \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "                               \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "                               \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\n",
    "                               \"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\"]])\n",
    "most_stable_full = (cv_all.assign(StabilityRank=lambda d: d[\"cv_auc_mean\"].rank(ascending=True, method=\"min\"))\n",
    "                    .sort_values([\"cv_auc_mean\",\"cv_auc_ndvi\",\"cv_auc_evi\"], ascending=[True,True,True])\n",
    "                    .loc[:, [\"ADM2_PCODE\",\"StabilityRank\",\"cv_auc_mean\",\"cv_auc_ndvi\",\"cv_auc_evi\"]])\n",
    "\n",
    "# Year winners (top 5 per year for narrative)\n",
    "rows = []\n",
    "for yr, dfy in metrics.groupby(\"year\"):\n",
    "    sc = per_year_score(dfy)\n",
    "    dfy2 = dfy.assign(YearScore=sc, YearRank=sc.rank(ascending=False, method=\"min\"))\n",
    "    rows.append(dfy2[[\"ADM2_PCODE\",\"year\",\"YearScore\",\"YearRank\"]])\n",
    "year_winners = (pd.concat(rows, ignore_index=True)\n",
    "                .sort_values([\"year\",\"YearRank\"]).groupby(\"year\").head(5).reset_index(drop=True))\n",
    "\n",
    "# Save workbook & CSVs\n",
    "xlsx_path = OUTPUT_DIR / \"adm2_category_rankings.xlsx\"\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as xw:\n",
    "    overall_leaders_full.to_excel(xw, sheet_name=\"Overall_Leaders\", index=False)\n",
    "    top_baseline_full.to_excel(xw, sheet_name=\"Top_Baseline\", index=False)\n",
    "    top_improvers_full.to_excel(xw, sheet_name=\"Top_Improvers\", index=False)\n",
    "    most_stable_full.to_excel(xw, sheet_name=\"Most_Stable\", index=False)\n",
    "    year_winners.to_excel(xw, sheet_name=\"Year_Winners\", index=False)\n",
    "\n",
    "overall_leaders_full.to_csv(OUTPUT_DIR / \"adm2_overall_leaders.csv\", index=False)\n",
    "top_baseline_full.to_csv(OUTPUT_DIR / \"adm2_top_baseline.csv\", index=False)\n",
    "top_improvers_full.to_csv(OUTPUT_DIR / \"adm2_top_improvers.csv\", index=False)\n",
    "most_stable_full.to_csv(OUTPUT_DIR / \"adm2_most_stable.csv\", index=False)\n",
    "print(\"Saved category outputs to:\", xlsx_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9c6bd-1039-48cb-89cc-aee8547bc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 14) Seasonal overlays & non-overlaid timeseries (using estimated windows) ----------\n",
    "hw_map = harv_windows.set_index(\"ADM2_PCODE\").to_dict(orient=\"index\")\n",
    "for adm2 in tqdm(sorted(data[\"ADM2_PCODE\"].unique()), desc=\"Seasonal overlays + time series\"):\n",
    "    sub = data.loc[data[\"ADM2_PCODE\"]==adm2, [\"ADM2_PCODE\",\"date\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"]].copy()\n",
    "    if sub.empty: continue\n",
    "    # NDVI windows\n",
    "    nd_s = hw_map.get(adm2, {}).get(\"ndvi_start_month\", np.nan)\n",
    "    nd_e = hw_map.get(adm2, {}).get(\"ndvi_end_month\",   np.nan)\n",
    "    seasonal_overlay(sub[[\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\"]].dropna(subset=[\"mean_NDVI\"]),\n",
    "                     \"mean_NDVI\", nd_s, nd_e, OVERLAY_NDVI_DIR / f\"{adm2}_seasonal_overlay_ndvi.png\")\n",
    "    timeseries_with_shading(sub[[\"ADM2_PCODE\",\"date\",\"year\",\"mean_NDVI\"]].dropna(subset=[\"mean_NDVI\"]),\n",
    "                            \"mean_NDVI\", nd_s, nd_e, TS_NDVI_DIR / f\"{adm2}_timeseries_ndvi.png\")\n",
    "    # EVI windows\n",
    "    ev_s = hw_map.get(adm2, {}).get(\"evi_start_month\", np.nan)\n",
    "    ev_e = hw_map.get(adm2, {}).get(\"evi_end_month\",   np.nan)\n",
    "    seasonal_overlay(sub[[\"ADM2_PCODE\",\"year\",\"month\",\"mean_EVI\"]].dropna(subset=[\"mean_EVI\"]),\n",
    "                     \"mean_EVI\", ev_s, ev_e, OVERLAY_EVI_DIR / f\"{adm2}_seasonal_overlay_evi.png\")\n",
    "    timeseries_with_shading(sub[[\"ADM2_PCODE\",\"date\",\"year\",\"mean_EVI\"]].dropna(subset=[\"mean_EVI\"]),\n",
    "                            \"mean_EVI\", ev_s, ev_e, TS_EVI_DIR / f\"{adm2}_timeseries_evi.png\")\n",
    "\n",
    "print(\"Done. Plots in:\")\n",
    "print(\"  \", OVERLAY_NDVI_DIR)\n",
    "print(\"  \", OVERLAY_EVI_DIR)\n",
    "print(\"  \", TS_NDVI_DIR)\n",
    "print(\"  \", TS_EVI_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ca3c3-d211-41ca-9e87-83102ac5f5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
