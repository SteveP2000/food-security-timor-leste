{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad2444b-2619-4966-a716-c99efd29de22",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58657d6-81c5-4b08-aa99-6bc88e824bfb",
   "metadata": {},
   "source": [
    "### 1) Setup & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f032a7-b0d5-49b1-9539-22e0dcf16581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "# INPUT: folder containing files like TL0101_NDVI_EVI_monthly_admn2.csv\n",
    "INPUT_DIR  = Path(r\"C:\\temp\\timor_leste\\ndvi_evi\")\n",
    "OUTPUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "PLOTS_DIR  = OUTPUT_DIR / \"plots\"\n",
    "\n",
    "# Category subfolders for plots\n",
    "PLOT_DIR_AUC_NDVI        = PLOTS_DIR / \"auc_ndvi\"\n",
    "PLOT_DIR_AUC_EVI         = PLOTS_DIR / \"auc_evi\"\n",
    "PLOT_DIR_HARV_MEAN_NDVI  = PLOTS_DIR / \"harvest_mean_ndvi\"\n",
    "PLOT_DIR_HARV_MEAN_EVI   = PLOTS_DIR / \"harvest_mean_evi\"\n",
    "\n",
    "# Make dirs\n",
    "for d in [OUTPUT_DIR, PLOTS_DIR, PLOT_DIR_AUC_NDVI, PLOT_DIR_AUC_EVI, PLOT_DIR_HARV_MEAN_NDVI, PLOT_DIR_HARV_MEAN_EVI]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config\n",
    "BASELINE_YEARS  = [2019, 2020, 2021]\n",
    "ANALYSIS_YEARS  = [2022, 2023, 2024, 2025]\n",
    "\n",
    "# New: where the estimated windows live\n",
    "HARVEST_WIN_CSV = OUTPUT_DIR / \"estimated_harvest_windows.csv\"\n",
    "USE_CONSENSUS_FOR_BOTH = False  # if True, use consensus window for both NDVI/EVI harvest means\n",
    "\n",
    "pd.set_option(\"display.width\", 140)\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7afda0-fd07-409d-bf36-1a22e42a8829",
   "metadata": {},
   "source": [
    "### 2) Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c96768-2a53-44cc-b2c9-562ca491d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "def _read_one_csv(fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'date' column in {fp}\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"days_in_month\"] = df[\"date\"].dt.days_in_month\n",
    "\n",
    "    # Infer ADM2 from filename if needed\n",
    "    adm2_guess = Path(fp).name.split(\"_\")[0]\n",
    "    if \"ADM2_PCODE\" not in df.columns:\n",
    "        df[\"ADM2_PCODE\"] = adm2_guess\n",
    "    else:\n",
    "        df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].fillna(adm2_guess)\n",
    "\n",
    "    # Cast numerics\n",
    "    for col in [\"mean_NDVI\",\"max_NDVI\",\"mean_EVI\",\"max_EVI\",\n",
    "                \"clear_frac_mean\",\"clear_frac_max\",\"count_images\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def load_monthly_files(input_dir: Path, max_workers: int = 8) -> pd.DataFrame:\n",
    "    files = glob.glob(str(input_dir / \"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No CSVs found in {input_dir}\")\n",
    "    frames = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_read_one_csv, fp): fp for fp in files}\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Loading CSVs\"):\n",
    "            fp = futures[f]\n",
    "            try:\n",
    "                frames.append(f.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {fp}: {e}\")\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# ---- harvest windows helpers ----\n",
    "def _months_in_span(start_m: int, end_m: int) -> set:\n",
    "    \"\"\"Return a set of months {1..12} covered by a start/end month (handles wrap).\"\"\"\n",
    "    if pd.isna(start_m) or pd.isna(end_m):\n",
    "        return set()\n",
    "    start_m, end_m = int(start_m), int(end_m)\n",
    "    if 1 <= start_m <= 12 and 1 <= end_m <= 12:\n",
    "        if start_m <= end_m:\n",
    "            return set(range(start_m, end_m + 1))\n",
    "        else:\n",
    "            return set(list(range(start_m, 13)) + list(range(1, end_m + 1)))\n",
    "    return set()\n",
    "\n",
    "def _load_harvest_windows(csv_path: Path) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Harvest windows CSV not found: {csv_path}\")\n",
    "    hw = pd.read_csv(csv_path)\n",
    "    need = {\"ADM2_PCODE\",\"ndvi_start_month\",\"ndvi_end_month\",\"evi_start_month\",\"evi_end_month\",\n",
    "            \"consensus_start_month\",\"consensus_end_month\"}\n",
    "    missing = need - set(hw.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{csv_path} missing columns: {missing}\")\n",
    "    return hw\n",
    "\n",
    "# ---- analytics helpers ----\n",
    "def _ensure_calendar_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        if \"year\" not in df.columns:\n",
    "            df[\"year\"] = df[\"date\"].dt.year\n",
    "        if \"month\" not in df.columns:\n",
    "            df[\"month\"] = df[\"date\"].dt.month\n",
    "        if \"days_in_month\" not in df.columns:\n",
    "            df[\"days_in_month\"] = df[\"date\"].dt.days_in_month\n",
    "    else:\n",
    "        # fallback if only year/month exist\n",
    "        need = {\"year\",\"month\"}\n",
    "        if need.issubset(df.columns):\n",
    "            if \"days_in_month\" not in df.columns:\n",
    "                df[\"days_in_month\"] = [\n",
    "                    calendar.monthrange(int(y), int(m))[1] if pd.notna(y) and pd.notna(m) else np.nan\n",
    "                    for y, m in zip(df[\"year\"], df[\"month\"])\n",
    "                ]\n",
    "        else:\n",
    "            raise KeyError(\"Need 'date' or both 'year' & 'month' to compute days_in_month.\")\n",
    "    return df\n",
    "\n",
    "def annual_auc(df: pd.DataFrame, index_col: str) -> pd.DataFrame:\n",
    "    df = _ensure_calendar_cols(df)\n",
    "    cols = [\"ADM2_PCODE\", \"year\", \"days_in_month\", index_col]\n",
    "    tmp = df.loc[:, cols].copy()\n",
    "\n",
    "    tmp[index_col] = pd.to_numeric(tmp[index_col], errors=\"coerce\")\n",
    "    tmp[\"days_in_month\"] = pd.to_numeric(tmp[\"days_in_month\"], errors=\"coerce\")\n",
    "\n",
    "    tmp[\"w\"] = tmp[index_col] * tmp[\"days_in_month\"]\n",
    "    out = (tmp.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[\"w\"].sum()\n",
    "             .rename(columns={\"w\": f\"AUC_{index_col}\"}))\n",
    "    return out\n",
    "\n",
    "def annual_auc_clearweighted(df: pd.DataFrame, index_col: str) -> pd.DataFrame:\n",
    "    if \"clear_frac_mean\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"ADM2_PCODE\",\"year\",f\"AUCcw_{index_col}\"])\n",
    "    df = _ensure_calendar_cols(df)\n",
    "    cols = [\"ADM2_PCODE\",\"year\",\"days_in_month\",index_col,\"clear_frac_mean\"]\n",
    "    tmp = df.loc[:, cols].copy()\n",
    "\n",
    "    tmp[index_col]       = pd.to_numeric(tmp[index_col], errors=\"coerce\")\n",
    "    tmp[\"days_in_month\"] = pd.to_numeric(tmp[\"days_in_month\"], errors=\"coerce\")\n",
    "    tmp[\"clear_frac_mean\"] = pd.to_numeric(tmp[\"clear_frac_mean\"], errors=\"coerce\").fillna(0.0).clip(0,1)\n",
    "\n",
    "    tmp[\"w\"] = tmp[index_col] * tmp[\"days_in_month\"] * tmp[\"clear_frac_mean\"]\n",
    "    out = (tmp.groupby([\"ADM2_PCODE\",\"year\"], as_index=False)[\"w\"].sum()\n",
    "             .rename(columns={\"w\": f\"AUCcw_{index_col}\"}))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def harvest_means_estimated(df: pd.DataFrame, index_col: str,\n",
    "                            hw_df: pd.DataFrame, which: str = \"ndvi\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mean over the **estimated** harvest window per ADM2/year.\n",
    "    which: \"ndvi\", \"evi\", or \"consensus\"\n",
    "    Output column keeps the same name as before: harv_{index_col}\n",
    "    \"\"\"\n",
    "    if which not in {\"ndvi\",\"evi\",\"consensus\"}:\n",
    "        raise ValueError(\"which must be 'ndvi', 'evi', or 'consensus'\")\n",
    "    # Build month sets per ADM2\n",
    "    hw_df = hw_df.copy()\n",
    "    if which == \"ndvi\":\n",
    "        msets = {r.ADM2_PCODE: _months_in_span(r.ndvi_start_month, r.ndvi_end_month) for r in hw_df.itertuples()}\n",
    "    elif which == \"evi\":\n",
    "        msets = {r.ADM2_PCODE: _months_in_span(r.evi_start_month, r.evi_end_month) for r in hw_df.itertuples()}\n",
    "    else:\n",
    "        msets = {r.ADM2_PCODE: _months_in_span(r.consensus_start_month, r.consensus_end_month) for r in hw_df.itertuples()}\n",
    "\n",
    "    # Tag rows that fall inside each ADM2's harvest months\n",
    "    df2 = df[[\"ADM2_PCODE\",\"year\",\"month\",index_col]].copy()\n",
    "    df2[\"__in_harv__\"] = df2.apply(lambda r: r[\"month\"] in msets.get(r[\"ADM2_PCODE\"], set()), axis=1)\n",
    "    sub = df2[df2[\"__in_harv__\"]].drop(columns=\"__in_harv__\")\n",
    "    out = sub.groupby([\"ADM2_PCODE\",\"year\"])[index_col].mean().reset_index(name=f\"harv_{index_col}\")\n",
    "    return out\n",
    "\n",
    "def slope_per_adm2(df: pd.DataFrame, value_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Simple linear slope per year per ADM2.\"\"\"\n",
    "    rows = []\n",
    "    for adm2, sub in df[[\"ADM2_PCODE\",\"year\",value_col]].dropna().groupby(\"ADM2_PCODE\"):\n",
    "        x = sub[\"year\"].values.astype(float)\n",
    "        y = sub[value_col].values.astype(float)\n",
    "        slope = np.polyfit(x, y, 1)[0] if len(np.unique(x)) >= 2 else np.nan\n",
    "        rows.append({\"ADM2_PCODE\": adm2, f\"slope_{value_col}\": slope})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_metric_by_adm2(df: pd.DataFrame, value_col: str, title: str, out_png: Path, ylim=None):\n",
    "    \"\"\"Multi-ADM2 line plot with optional fixed y-limits.\"\"\"\n",
    "    plt.figure()\n",
    "    for adm2, sub in df.sort_values([\"ADM2_PCODE\",\"year\"]).groupby(\"ADM2_PCODE\"):\n",
    "        plt.plot(sub[\"year\"], sub[value_col], label=adm2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Year\"); plt.ylabel(value_col)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.legend(ncols=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png)\n",
    "    plt.close()\n",
    "\n",
    "def plot_per_adm2(df: pd.DataFrame, value_col: str, out_dir: Path, ylim=None, title_prefix:str=\"\"):\n",
    "    \"\"\"One PNG per ADM2 with consistent y-limit, saved to out_dir.\"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for adm2, sub in tqdm(df.groupby(\"ADM2_PCODE\"), total=df[\"ADM2_PCODE\"].nunique(), desc=f\"Plots: {value_col}\"):\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"year\"], sub[value_col])\n",
    "        plt.title(f\"{title_prefix}{adm2}\")\n",
    "        plt.xlabel(\"Year\"); plt.ylabel(value_col)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / f\"{adm2}_{value_col}.png\")\n",
    "        plt.close()\n",
    "\n",
    "def padded_limits(series: pd.Series, pad: float = 0.05):\n",
    "    \"\"\"Compute padded [min,max] for AUC-type series; handles NaNs.\"\"\"\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return (0, 1)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if lo == hi:\n",
    "        span = abs(hi) if hi != 0 else 1.0\n",
    "        lo, hi = hi - 0.1*span, hi + 0.1*span\n",
    "    pad_span = (hi - lo) * pad\n",
    "    return (lo - pad_span, hi + pad_span)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201b44d-ab5f-488d-aa45-f22e02911df5",
   "metadata": {},
   "source": [
    "### 3) Create harvest window estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46628971-8781-4033-aee4-8215450a074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Estimate harvest window per ADM2 from NDVI/EVI seasonality\n",
    "# ============================================================\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Paths / I/O ----------\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "if 'PLOTS_DIR' not in globals():\n",
    "    PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where the monthly CSVs live, if we need to reload:\n",
    "if 'INPUT_DIR' in globals():\n",
    "    MONTHLY_DIR = Path(INPUT_DIR)\n",
    "else:\n",
    "    MONTHLY_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi\")\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"estimated_harvest_windows.csv\"\n",
    "\n",
    "# Optional annotated plots\n",
    "MAKE_ANNOTATED_PLOTS = True\n",
    "PLOT_DIR_CONS   = PLOTS_DIR / \"harvest_estimate_consensus\"\n",
    "PLOT_DIR_NDVI   = PLOTS_DIR / \"harvest_estimate_ndvi\"\n",
    "PLOT_DIR_EVI    = PLOTS_DIR / \"harvest_estimate_evi\"\n",
    "for d in [PLOT_DIR_CONS, PLOT_DIR_NDVI, PLOT_DIR_EVI]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Config ----------\n",
    "BASELINE_YEARS = [2019, 2020, 2021]\n",
    "RECENT_YEARS   = [2022, 2023, 2024, 2025]\n",
    "MIN_CLEAR_FRAC = 0.20   # treat months below this clear fraction as unreliable (masked)\n",
    "Y_LIMIT = (0, 1)\n",
    "\n",
    "# ---------- Load monthly data if needed ----------\n",
    "def _read_one_csv(fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'date' column in {fp}\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    adm2_guess = Path(fp).name.split(\"_\")[0]\n",
    "    if \"ADM2_PCODE\" not in df.columns:\n",
    "        df[\"ADM2_PCODE\"] = adm2_guess\n",
    "    else:\n",
    "        df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].fillna(adm2_guess)\n",
    "    for col in [\"mean_NDVI\",\"mean_EVI\",\"clear_frac_mean\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _load_monthly_if_needed():\n",
    "    if 'data' in globals():\n",
    "        need = {\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"}\n",
    "        if isinstance(data, pd.DataFrame) and need.issubset(data.columns):\n",
    "            return data\n",
    "    files = glob.glob(str(MONTHLY_DIR / \"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No monthly CSVs found in {MONTHLY_DIR}\")\n",
    "    frames = []\n",
    "    for fp in tqdm(files, desc=\"Loading monthly CSVs\"):\n",
    "        try:\n",
    "            frames.append(_read_one_csv(fp))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No valid monthly CSVs loaded.\")\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "data = _load_monthly_if_needed()\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def _cyclic_roll(arr, shift):\n",
    "    shift = shift % len(arr)\n",
    "    if shift == 0: return arr.copy()\n",
    "    return np.concatenate([arr[shift:], arr[:shift]])\n",
    "\n",
    "def _smooth_cyclic(vals, window=3):\n",
    "    \"\"\"Centered moving average with cyclic wrap (expects len=12).\"\"\"\n",
    "    assert len(vals) == 12\n",
    "    x = vals.astype(float).copy()\n",
    "    # simple pad with wrap:\n",
    "    padded = np.r_[x[-(window//2):], x, x[:(window//2)]]\n",
    "    sm = np.convolve(padded, np.ones(window)/window, mode='valid')\n",
    "    return sm  # length 12\n",
    "\n",
    "def _interp_cyclic(months, vals):\n",
    "    \"\"\"Interpolate NaNs on cyclic month axis.\"\"\"\n",
    "    m = np.array(months, dtype=float)\n",
    "    v = vals.astype(float).copy()\n",
    "    isnan = np.isnan(v)\n",
    "    if isnan.all():\n",
    "        return v\n",
    "    # add wrap points (0 with Dec, 13 with Jan) to help endpoints\n",
    "    m_ext = np.r_[0, m, 13]\n",
    "    v_ext = np.r_[v[11], v, v[0]]\n",
    "    good = ~np.isnan(v_ext)\n",
    "    v_interp = np.interp(m_ext, m_ext[good], v_ext[good])\n",
    "    out = v_interp[1:-1]\n",
    "    return out\n",
    "\n",
    "def _estimate_window_from_series(months, vals, clear=None):\n",
    "    \"\"\"\n",
    "    months: 1..12\n",
    "    vals: NDVI/EVI length-12 monthly mean\n",
    "    clear: optional length-12 clear frac (0..1)\n",
    "    Returns dict with start_month, end_month, peak_month, trough_month, range, confidence, method\n",
    "    \"\"\"\n",
    "    months = np.array(months, dtype=int)\n",
    "    x = np.array(vals, dtype=float)\n",
    "\n",
    "    # Mask very low-clear months\n",
    "    if clear is not None:\n",
    "        c = np.array(clear, dtype=float)\n",
    "        x = np.where((~np.isnan(c)) & (c < MIN_CLEAR_FRAC), np.nan, x)\n",
    "\n",
    "    # Fill missing by cyclic interpolation, then smooth\n",
    "    x_filled = _interp_cyclic(months, x)\n",
    "    x_sm = _smooth_cyclic(x_filled, window=3)\n",
    "\n",
    "    # Peak month (tie -> earliest)\n",
    "    peak_idx = int(np.nanargmax(x_sm))\n",
    "    peak_month = int(months[peak_idx])\n",
    "\n",
    "    # Find trough within 6 months after peak (search in rolled space)\n",
    "    rolled = _cyclic_roll(x_sm, peak_idx)  # starts at peak month\n",
    "    # search next 1..6 (avoid the peak itself)\n",
    "    search_slice = rolled[1:7]\n",
    "    trough_rel = int(np.nanargmin(search_slice)) + 1\n",
    "    trough_idx = (peak_idx + trough_rel) % 12\n",
    "    trough_month = int(months[trough_idx])\n",
    "\n",
    "    peak = x_sm[peak_idx]\n",
    "    trough = x_sm[trough_idx]\n",
    "    R = float(peak - trough)\n",
    "\n",
    "    # thresholds\n",
    "    T1 = peak - 0.20 * R  # onset when 20% down from peak\n",
    "    T2 = peak - 0.60 * R  # end when 60% down from peak\n",
    "\n",
    "    # scan forward for onset/end\n",
    "    start_idx = None\n",
    "    end_idx   = None\n",
    "    for k in range(1, 7):  # within 6 months after peak\n",
    "        idx = (peak_idx + k) % 12\n",
    "        if start_idx is None and x_sm[idx] <= T1:\n",
    "            start_idx = idx\n",
    "        if start_idx is not None and x_sm[idx] <= T2:\n",
    "            end_idx = idx\n",
    "            break\n",
    "\n",
    "    # Fallbacks if thresholds not crossed (low contrast etc.)\n",
    "    method = \"percent_of_range\"\n",
    "    if start_idx is None or end_idx is None:\n",
    "        method = \"steepest_decline_fallback\"\n",
    "        diffs = np.r_[np.diff(x_sm), x_sm[0]-x_sm[-1]]  # month-to-month cyclic\n",
    "        # Look for steepest negative slope after the peak\n",
    "        idxs = [(peak_idx + k) % 12 for k in range(1, 7)]\n",
    "        neg_slopes = diffs[idxs]\n",
    "        steep_rel = int(np.nanargmin(neg_slopes)) + 1\n",
    "        center_idx = (peak_idx + steep_rel) % 12\n",
    "        start_idx = start_idx or center_idx\n",
    "        end_idx   = end_idx   or ((center_idx + 1) % 12)\n",
    "\n",
    "    # Normalize ordering across year wrap\n",
    "    # We'll output calendar months (1..12); if end < start, interpret as wrapping across Aug->Oct etc.\n",
    "    start_month = int(months[start_idx])\n",
    "    end_month   = int(months[end_idx])\n",
    "\n",
    "    # Confidence: higher with larger R and more monotonic drop\n",
    "    # monotonic score over the descent window\n",
    "    seq = []\n",
    "    i = start_idx\n",
    "    while True:\n",
    "        j = (i + 1) % 12\n",
    "        seq.append(x_sm[j] - x_sm[i])\n",
    "        i = j\n",
    "        if i == end_idx:\n",
    "            break\n",
    "    monotone = float(np.mean(np.array(seq) < 0)) if seq else 0.0\n",
    "    conf = np.clip((R / 0.35) * (0.5 + 0.5 * monotone), 0, 1)\n",
    "\n",
    "    return dict(\n",
    "        start_month=start_month,\n",
    "        end_month=end_month,\n",
    "        peak_month=peak_month,\n",
    "        trough_month=trough_month,\n",
    "        range_R=round(R, 3),\n",
    "        confidence=round(conf, 3),\n",
    "        method=method\n",
    "    )\n",
    "\n",
    "def _build_recent_climatology(df_adm2, value_col):\n",
    "    \"\"\"Return arrays (months 1..12) of recent-year monthly means (prefers 2022–2025).\"\"\"\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    # prefer recent years if present\n",
    "    yrs_present = sorted(df_adm2[\"year\"].unique().tolist())\n",
    "    yrs_recent = [y for y in RECENT_YEARS if y in yrs_present]\n",
    "    if len(yrs_recent) >= 2:\n",
    "        use_years = yrs_recent\n",
    "    else:\n",
    "        use_years = yrs_present  # fallback: all years available\n",
    "    piv = (df_adm2[df_adm2[\"year\"].isin(use_years)]\n",
    "           .pivot_table(index=\"month\", values=value_col, aggfunc=\"mean\"))\n",
    "    series = piv.reindex(months).values.ravel()\n",
    "    return months, series\n",
    "\n",
    "def _build_recent_clear(df_adm2):\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    if \"clear_frac_mean\" not in df_adm2.columns:\n",
    "        return months, np.full(12, np.nan)\n",
    "    piv = (df_adm2\n",
    "           .pivot_table(index=\"month\", values=\"clear_frac_mean\", aggfunc=\"mean\"))\n",
    "    series = piv.reindex(months).values.ravel()\n",
    "    return months, series\n",
    "\n",
    "# ---------- Main loop ----------\n",
    "records = []\n",
    "adm2_list = sorted(data[\"ADM2_PCODE\"].dropna().unique().tolist())\n",
    "\n",
    "for adm2 in tqdm(adm2_list, desc=\"Estimating harvest windows\"):\n",
    "    sub = data.loc[data[\"ADM2_PCODE\"] == adm2,\n",
    "                   [\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\",\"clear_frac_mean\"]].copy()\n",
    "\n",
    "    m_ndvi, s_ndvi = _build_recent_climatology(sub, \"mean_NDVI\")\n",
    "    m_evi,  s_evi  = _build_recent_climatology(sub, \"mean_EVI\")\n",
    "    m_clr,  s_clr  = _build_recent_clear(sub)\n",
    "\n",
    "    ndvi_est = _estimate_window_from_series(m_ndvi, s_ndvi, clear=s_clr)\n",
    "    evi_est  = _estimate_window_from_series(m_evi,  s_evi,  clear=s_clr)\n",
    "\n",
    "    # Consensus = union (min start, max end). If wrap-around (e.g., start 11, end 2), normalize to May–Aug style if possible.\n",
    "    # Simple union in month numbers:\n",
    "    def _span_to_set(start, end):\n",
    "        if start <= end:\n",
    "            return set(range(start, end+1))\n",
    "        else:\n",
    "            return set(list(range(start, 13)) + list(range(1, end+1)))\n",
    "\n",
    "    ndvi_set = _span_to_set(ndvi_est[\"start_month\"], ndvi_est[\"end_month\"])\n",
    "    evi_set  = _span_to_set(evi_est[\"start_month\"],  evi_est[\"end_month\"])\n",
    "    union    = sorted(list(ndvi_set.union(evi_set)))\n",
    "    if not union:\n",
    "        cons_start, cons_end = ndvi_est[\"start_month\"], ndvi_est[\"end_month\"]\n",
    "        cons_method = \"NDVI_only\"\n",
    "    else:\n",
    "        # merge contiguous wrap-aware\n",
    "        # Heuristic: prefer a window length 2–4 months around mid-year if present\n",
    "        cons_start, cons_end = union[0], union[-1]\n",
    "        cons_method = \"union(NDVI,EVI)\"\n",
    "\n",
    "    records.append({\n",
    "        \"ADM2_PCODE\": adm2,\n",
    "        # NDVI-based\n",
    "        \"ndvi_start_month\": ndvi_est[\"start_month\"],\n",
    "        \"ndvi_end_month\":   ndvi_est[\"end_month\"],\n",
    "        \"ndvi_peak_month\":  ndvi_est[\"peak_month\"],\n",
    "        \"ndvi_trough_month\":ndvi_est[\"trough_month\"],\n",
    "        \"ndvi_range\":       ndvi_est[\"range_R\"],\n",
    "        \"ndvi_confidence\":  ndvi_est[\"confidence\"],\n",
    "        \"ndvi_method\":      ndvi_est[\"method\"],\n",
    "        # EVI-based\n",
    "        \"evi_start_month\":  evi_est[\"start_month\"],\n",
    "        \"evi_end_month\":    evi_est[\"end_month\"],\n",
    "        \"evi_peak_month\":   evi_est[\"peak_month\"],\n",
    "        \"evi_trough_month\": evi_est[\"trough_month\"],\n",
    "        \"evi_range\":        evi_est[\"range_R\"],\n",
    "        \"evi_confidence\":   evi_est[\"confidence\"],\n",
    "        \"evi_method\":       evi_est[\"method\"],\n",
    "        # Consensus\n",
    "        \"consensus_start_month\": cons_start,\n",
    "        \"consensus_end_month\":   cons_end,\n",
    "        \"consensus_method\":      cons_method\n",
    "    })\n",
    "\n",
    "# Save CSV\n",
    "harvest_df = pd.DataFrame.from_records(records)\n",
    "harvest_df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved harvest window estimates to:\", OUT_CSV)\n",
    "\n",
    "# ---------- Optional: annotated plots ----------\n",
    "def _month_labels():\n",
    "    return [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "def _plot_annotated(adm2, months, series, label, start_m, end_m, out_png):\n",
    "    plt.figure()\n",
    "    plt.plot(months, series, label=label)\n",
    "    # Shade suggested harvest\n",
    "    if start_m <= end_m:\n",
    "        plt.axvspan(start_m-0.5, end_m+0.5, alpha=0.12)\n",
    "    else:\n",
    "        # wrap case\n",
    "        plt.axvspan(0.5, end_m+0.5, alpha=0.12)\n",
    "        plt.axvspan(start_m-0.5, 12.5, alpha=0.12)\n",
    "    # Cosmetics\n",
    "    plt.xticks(months, _month_labels())\n",
    "    plt.ylim(*Y_LIMIT)\n",
    "    plt.xlabel(\"Month\"); plt.ylabel(label)\n",
    "    plt.title(f\"{adm2} — Estimated harvest: {start_m:02d}–{end_m:02d}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png)\n",
    "    plt.close()\n",
    "\n",
    "if MAKE_ANNOTATED_PLOTS:\n",
    "    for row in tqdm(harvest_df.itertuples(index=False), total=len(harvest_df), desc=\"Annotated plots\"):\n",
    "        adm2 = row.ADM2_PCODE\n",
    "        sub = data.loc[data[\"ADM2_PCODE\"] == adm2, [\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"]]\n",
    "        m_ndvi, s_ndvi = _build_recent_climatology(sub, \"mean_NDVI\")\n",
    "        m_evi,  s_evi  = _build_recent_climatology(sub, \"mean_EVI\")\n",
    "\n",
    "        # NDVI\n",
    "        _plot_annotated(adm2, m_ndvi, _smooth_cyclic(_interp_cyclic(m_ndvi, s_ndvi), 3),\n",
    "                        \"NDVI\", row.ndvi_start_month, row.ndvi_end_month,\n",
    "                        PLOT_DIR_NDVI / f\"{adm2}_harvest_ndvi.png\")\n",
    "        # EVI\n",
    "        _plot_annotated(adm2, m_evi, _smooth_cyclic(_interp_cyclic(m_evi, s_evi), 3),\n",
    "                        \"EVI\",  row.evi_start_month, row.evi_end_month,\n",
    "                        PLOT_DIR_EVI / f\"{adm2}_harvest_evi.png\")\n",
    "        # Consensus (draw NDVI curve as reference)\n",
    "        _plot_annotated(adm2, m_ndvi, _smooth_cyclic(_interp_cyclic(m_ndvi, s_ndvi), 3),\n",
    "                        \"NDVI (consensus shading)\", row.consensus_start_month, row.consensus_end_month,\n",
    "                        PLOT_DIR_CONS / f\"{adm2}_harvest_consensus.png\")\n",
    "\n",
    "print(\"Annotated plots saved in:\")\n",
    "print(\" -\", PLOT_DIR_CONS)\n",
    "print(\" -\", PLOT_DIR_NDVI)\n",
    "print(\" -\", PLOT_DIR_EVI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f332ce-9472-471d-a4ad-0c93e1e72eac",
   "metadata": {},
   "source": [
    "### 3) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4bb48-0d9c-4469-9ea0-a1308f5cb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_monthly_files(INPUT_DIR, max_workers=12)\n",
    "print(\"Rows:\", len(data), \" | ADM2s:\", data['ADM2_PCODE'].nunique())\n",
    "data.head(3)\n",
    "\n",
    "harv_windows = _load_harvest_windows(HARVEST_WIN_CSV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba25a0-8b60-4451-91b9-ce4b54908847",
   "metadata": {},
   "source": [
    "### 4) Build annual metrics (AUCs & harvest-window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec75de0-42bf-4990-b430-f0ff6247e3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "auc_ndvi = annual_auc(data, \"mean_NDVI\")\n",
    "auc_evi  = annual_auc(data, \"mean_EVI\")\n",
    "auc_ndvi_cw = annual_auc_clearweighted(data, \"mean_NDVI\")\n",
    "auc_evi_cw  = annual_auc_clearweighted(data, \"mean_EVI\")\n",
    "\n",
    "if USE_CONSENSUS_FOR_BOTH:\n",
    "    harv_ndvi = harvest_means_estimated(data, \"mean_NDVI\", harv_windows, which=\"consensus\")\n",
    "    harv_evi  = harvest_means_estimated(data, \"mean_EVI\",  harv_windows, which=\"consensus\")\n",
    "else:\n",
    "    # NDVI → NDVI window; EVI → EVI window\n",
    "    harv_ndvi = harvest_means_estimated(data, \"mean_NDVI\", harv_windows, which=\"ndvi\")\n",
    "    harv_evi  = harvest_means_estimated(data, \"mean_EVI\",  harv_windows, which=\"evi\")\n",
    "\n",
    "metrics = (auc_ndvi.merge(auc_evi, on=[\"ADM2_PCODE\",\"year\"], how=\"outer\")\n",
    "                    .merge(auc_ndvi_cw, on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(auc_evi_cw,  on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(harv_ndvi,   on=[\"ADM2_PCODE\",\"year\"], how=\"left\")\n",
    "                    .merge(harv_evi,    on=[\"ADM2_PCODE\",\"year\"], how=\"left\"))\n",
    "\n",
    "metrics.sort_values([\"ADM2_PCODE\",\"year\"]).head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fde3a-bba1-4de2-b993-4c7ae07a5d1b",
   "metadata": {},
   "source": [
    "### 5) Baseline (2019–2021) & anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7584cc2-ba25-4576-8153-7208a99b7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = (\n",
    "    metrics[metrics[\"year\"].isin(BASELINE_YEARS)]\n",
    "    .groupby(\"ADM2_PCODE\")\n",
    "    .agg({\n",
    "        \"AUC_mean_NDVI\":\"mean\", \"AUC_mean_EVI\":\"mean\",\n",
    "        \"harv_mean_NDVI\":\"mean\",\"harv_mean_EVI\":\"mean\",\n",
    "        \"AUCcw_mean_NDVI\":\"mean\",\"AUCcw_mean_EVI\":\"mean\",\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"AUC_mean_NDVI\":\"base_AUC_NDVI\",\n",
    "        \"AUC_mean_EVI\":\"base_AUC_EVI\",\n",
    "        \"harv_mean_NDVI\":\"base_harv_NDVI\",\n",
    "        \"harv_mean_EVI\":\"base_harv_EVI\",\n",
    "        \"AUCcw_mean_NDVI\":\"base_AUCcw_NDVI\",\n",
    "        \"AUCcw_mean_EVI\":\"base_AUCcw_EVI\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "metrics = metrics.merge(base, on=\"ADM2_PCODE\", how=\"left\")\n",
    "\n",
    "for col, bcol in [\n",
    "    (\"AUC_mean_NDVI\",\"base_AUC_NDVI\"),\n",
    "    (\"AUC_mean_EVI\",\"base_AUC_EVI\"),\n",
    "    (\"harv_mean_NDVI\",\"base_harv_NDVI\"),\n",
    "    (\"harv_mean_EVI\",\"base_harv_EVI\"),\n",
    "    (\"AUCcw_mean_NDVI\",\"base_AUCcw_NDVI\"),\n",
    "    (\"AUCcw_mean_EVI\",\"base_AUCcw_EVI\"),\n",
    "]:\n",
    "    if col in metrics.columns and bcol in metrics.columns:\n",
    "        metrics[f\"{col}_anom\"] = metrics[col] - metrics[bcol]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            metrics[f\"{col}_anom_pct\"] = np.where(\n",
    "                (metrics[bcol].notna()) & (metrics[bcol].abs() > 0),\n",
    "                (metrics[f\"{col}_anom\"] / metrics[bcol]) * 100.0,\n",
    "                np.nan\n",
    "            )\n",
    "\n",
    "metrics.sort_values([\"ADM2_PCODE\",\"year\"]).head(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57de94-a0f6-4ab2-ad5c-b7b24bafa996",
   "metadata": {},
   "source": [
    "### 6) Trends & QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223275d-17d6-4b87-8bd5-26312e112677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_auc_ndvi = slope_per_adm2(metrics, \"AUC_mean_NDVI\")\n",
    "trend_auc_evi  = slope_per_adm2(metrics, \"AUC_mean_EVI\")\n",
    "trend_hndvi    = slope_per_adm2(metrics, \"harv_mean_NDVI\")\n",
    "trend_hevi     = slope_per_adm2(metrics, \"harv_mean_EVI\")\n",
    "\n",
    "trend = (trend_auc_ndvi.merge(trend_auc_evi, on=\"ADM2_PCODE\", how=\"outer\")\n",
    "                        .merge(trend_hndvi, on=\"ADM2_PCODE\", how=\"outer\")\n",
    "                        .merge(trend_hevi, on=\"ADM2_PCODE\", how=\"outer\"))\n",
    "\n",
    "# QA: average clear fraction across the **estimated** harvest months in analysis years\n",
    "if \"clear_frac_mean\" in data.columns:\n",
    "    hw_map_ndvi = {r.ADM2_PCODE: _months_in_span(r.ndvi_start_month, r.ndvi_end_month) for r in harv_windows.itertuples()}\n",
    "    hw_map_evi  = {r.ADM2_PCODE: _months_in_span(r.evi_start_month,  r.evi_end_month)  for r in harv_windows.itertuples()}\n",
    "    # use union of windows to be safe for QA\n",
    "    union_map = {k: (hw_map_ndvi.get(k,set()) | hw_map_evi.get(k,set())) for k in set(hw_map_ndvi)|set(hw_map_evi)}\n",
    "    dqa = data[data[\"year\"].isin(ANALYSIS_YEARS)].copy()\n",
    "    dqa[\"__in_hw_union__\"] = dqa.apply(lambda r: r[\"month\"] in union_map.get(r[\"ADM2_PCODE\"], set()), axis=1)\n",
    "    qa_summary = (dqa[dqa[\"__in_hw_union__\"]]\n",
    "                  .groupby(\"ADM2_PCODE\")[\"clear_frac_mean\"]\n",
    "                  .mean().reset_index(name=\"avg_clear_frac_harv_2022_2025\"))\n",
    "else:\n",
    "    qa_summary = pd.DataFrame({\"ADM2_PCODE\": metrics[\"ADM2_PCODE\"].unique(), \"avg_clear_frac_harv_2022_2025\": np.nan})\n",
    "\n",
    "# Final summary per ADM2 (averages for 2022–2025)\n",
    "summary = (\n",
    "    base.merge(\n",
    "        metrics[metrics[\"year\"].isin(ANALYSIS_YEARS)]\n",
    "        .groupby(\"ADM2_PCODE\")\n",
    "        .agg({\n",
    "            \"AUC_mean_NDVI_anom\":\"mean\",\n",
    "            \"AUC_mean_EVI_anom\":\"mean\",\n",
    "            \"AUC_mean_NDVI_anom_pct\":\"mean\",\n",
    "            \"AUC_mean_EVI_anom_pct\":\"mean\",\n",
    "            \"AUCcw_mean_NDVI_anom\":\"mean\",\n",
    "            \"AUCcw_mean_EVI_anom\":\"mean\",\n",
    "            \"AUCcw_mean_NDVI_anom_pct\":\"mean\",\n",
    "            \"AUCcw_mean_EVI_anom_pct\":\"mean\",\n",
    "            \"harv_mean_NDVI_anom\":\"mean\",\n",
    "            \"harv_mean_EVI_anom\":\"mean\",\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"AUC_mean_NDVI_anom\":\"avg_AUC_NDVI_anom_2022_2025\",\n",
    "            \"AUC_mean_EVI_anom\":\"avg_AUC_EVI_anom_2022_2025\",\n",
    "            \"AUC_mean_NDVI_anom_pct\":\"avg_AUC_NDVI_anom_pct_2022_2025\",\n",
    "            \"AUC_mean_EVI_anom_pct\":\"avg_AUC_EVI_anom_pct_2022_2025\",\n",
    "            \"AUCcw_mean_NDVI_anom\":\"avg_AUCcw_NDVI_anom_2022_2025\",\n",
    "            \"AUCcw_mean_EVI_anom\":\"avg_AUCcw_EVI_anom_2022_2025\",\n",
    "            \"AUCcw_mean_NDVI_anom_pct\":\"avg_AUCcw_NDVI_anom_pct_2022_2025\",\n",
    "            \"AUCcw_mean_EVI_anom_pct\":\"avg_AUCcw_EVI_anom_pct_2022_2025\",\n",
    "            \"harv_mean_NDVI_anom\":\"avg_harv_NDVI_anom_2022_2025\",\n",
    "            \"harv_mean_EVI_anom\":\"avg_harv_EVI_anom_2022_2025\",\n",
    "        }),\n",
    "        on=\"ADM2_PCODE\", how=\"left\"\n",
    "    )\n",
    "    .merge(trend, on=\"ADM2_PCODE\", how=\"left\")\n",
    "    .merge(qa_summary, on=\"ADM2_PCODE\", how=\"left\")\n",
    ")\n",
    "\n",
    "summary.round(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ccbd7-8846-48dc-a11c-b9f4ceda9a4d",
   "metadata": {},
   "source": [
    "### 7) Save CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c915f-5d3e-4d06-91d8-128a74c1c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = OUTPUT_DIR / \"panel_summary_by_ADM2.csv\"\n",
    "metrics_path = OUTPUT_DIR / \"panel_yearly_metrics_long.csv\"\n",
    "qa_path      = OUTPUT_DIR / \"panel_QA.csv\"\n",
    "\n",
    "summary.to_csv(summary_path, index=False)\n",
    "metrics.to_csv(metrics_path, index=False)\n",
    "qa_summary.to_csv(qa_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", summary_path)\n",
    "print(\"  \", metrics_path)\n",
    "print(\"  \", qa_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792b8c8-86d0-42a0-b2dc-295e40aed157",
   "metadata": {},
   "source": [
    "### 8) Aggregate plots (PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904534d8-7343-4f28-98e8-181ac7b83155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Determine consistent y-limits ----\n",
    "# AUC ranges come from data; harvest means fixed to [0,1]\n",
    "auc_ndvi_ylim = padded_limits(auc_ndvi[\"AUC_mean_NDVI\"], pad=0.08)\n",
    "auc_evi_ylim  = padded_limits(auc_evi[\"AUC_mean_EVI\"],   pad=0.08)\n",
    "harv_ndvi_ylim = (0, 1)\n",
    "harv_evi_ylim  = (0, 1)\n",
    "\n",
    "# ---- Aggregate multi-ADM2 charts (one per metric) ----\n",
    "plot_metric_by_adm2(auc_ndvi, \"AUC_mean_NDVI\",\n",
    "                    \"Annual AUC (mean NDVI) by ADM2\",\n",
    "                    PLOT_DIR_AUC_NDVI / \"overview_auc_ndvi.png\",\n",
    "                    ylim=auc_ndvi_ylim)\n",
    "\n",
    "plot_metric_by_adm2(auc_evi,  \"AUC_mean_EVI\",\n",
    "                    \"Annual AUC (mean EVI) by ADM2\",\n",
    "                    PLOT_DIR_AUC_EVI / \"overview_auc_evi.png\",\n",
    "                    ylim=auc_evi_ylim)\n",
    "\n",
    "plot_metric_by_adm2(harv_ndvi, \"harv_mean_NDVI\",\n",
    "                    \"Harvest-window Mean NDVI (estimated) by ADM2\",\n",
    "                    PLOT_DIR_HARV_MEAN_NDVI / \"overview_harvest_mean_ndvi.png\",\n",
    "                    ylim=harv_ndvi_ylim)\n",
    "\n",
    "plot_metric_by_adm2(harv_evi,  \"harv_mean_EVI\",\n",
    "                    \"Harvest-window Mean EVI (estimated) by ADM2\",\n",
    "                    PLOT_DIR_HARV_MEAN_EVI / \"overview_harvest_mean_evi.png\",\n",
    "                    ylim=harv_evi_ylim)\n",
    "\n",
    "print(\"Saved aggregate plots to:\")\n",
    "print(\" -\", PLOT_DIR_AUC_NDVI / \"overview_auc_ndvi.png\")\n",
    "print(\" -\", PLOT_DIR_AUC_EVI / \"overview_auc_evi.png\")\n",
    "print(\" -\", PLOT_DIR_HARV_MEAN_NDVI / \"overview_harvest_mean_ndvi.png\")\n",
    "print(\" -\", PLOT_DIR_HARV_MEAN_EVI / \"overview_harvest_mean_evi.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22a273-7b1e-4a14-858d-3100f9136a31",
   "metadata": {},
   "source": [
    "### 9) Per-ADM2 plots (optional, one PNG per ADM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e3630-010c-4f99-bfde-4e53a01982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same y-limits computed above for consistency across all ADM2 plots\n",
    "plot_per_adm2(auc_ndvi, \"AUC_mean_NDVI\",\n",
    "              out_dir=PLOT_DIR_AUC_NDVI,\n",
    "              ylim=auc_ndvi_ylim,\n",
    "              title_prefix=\"AUC (NDVI) — \")\n",
    "\n",
    "plot_per_adm2(auc_evi,  \"AUC_mean_EVI\",\n",
    "              out_dir=PLOT_DIR_AUC_EVI,\n",
    "              ylim=auc_evi_ylim,\n",
    "              title_prefix=\"AUC (EVI) — \")\n",
    "\n",
    "plot_per_adm2(harv_ndvi, \"harv_mean_NDVI\",\n",
    "              out_dir=PLOT_DIR_HARV_MEAN_NDVI,\n",
    "              ylim=harv_ndvi_ylim,\n",
    "              title_prefix=\"Harvest mean NDVI (estimated) — \")\n",
    "\n",
    "plot_per_adm2(harv_evi,  \"harv_mean_EVI\",\n",
    "              out_dir=PLOT_DIR_HARV_MEAN_EVI,\n",
    "              ylim=harv_evi_ylim,\n",
    "              title_prefix=\"Harvest mean EVI (estimated) — \")\n",
    "\n",
    "print(\"Saved per-ADM2 plots in:\")\n",
    "print(\" -\", PLOT_DIR_AUC_NDVI)\n",
    "print(\" -\", PLOT_DIR_AUC_EVI)\n",
    "print(\" -\", PLOT_DIR_HARV_MEAN_NDVI)\n",
    "print(\" -\", PLOT_DIR_HARV_MEAN_EVI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af66b65-d098-480d-88ae-8b2098d76854",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504934a-616c-463d-a62e-74c8ffec79b5",
   "metadata": {},
   "source": [
    "### 1) Imports, paths, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2fa26-a009-4d49-bf89-5d7ecfb6b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Paths ----\n",
    "OUT_DIR    = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "SUMMARY_CSV = OUT_DIR / \"panel_summary_by_ADM2.csv\"\n",
    "METRICS_CSV = OUT_DIR / \"panel_yearly_metrics_long.csv\"\n",
    "\n",
    "# ---- Helper: winsorized z-score (robust to outliers/scale) ----\n",
    "def zscore_winsor(s: pd.Series, p: float = 0.05) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.notna().sum() < 2:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    lo, hi = s.quantile([p, 1 - p])\n",
    "    s_clip = s.clip(lo, hi)\n",
    "    std = s_clip.std(ddof=0)\n",
    "    if std == 0 or np.isnan(std):\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    return (s_clip - s_clip.mean()) / std\n",
    "\n",
    "# Percentile rank (0–100)\n",
    "def pct_rank(s: pd.Series) -> pd.Series:\n",
    "    return 100 * s.rank(pct=True, method=\"average\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b1d39-af04-4203-a728-cf8a35819cb8",
   "metadata": {},
   "source": [
    "### 2) Load data & define score recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53677c-b196-41d1-82e6-35bd50a7c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your summary (ADM2-level) and yearly metrics (long)\n",
    "summary = pd.read_csv(SUMMARY_CSV)\n",
    "metrics = pd.read_csv(METRICS_CSV)\n",
    "\n",
    "# If any columns are missing (older runs), create placeholders\n",
    "for col in [\n",
    "    \"base_AUC_NDVI\", \"base_AUC_EVI\", \"base_harv_NDVI\", \"base_harv_EVI\",\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\", \"avg_AUC_EVI_anom_2022_2025\",\n",
    "    \"avg_harv_NDVI_anom_2022_2025\", \"avg_harv_EVI_anom_2022_2025\",\n",
    "    \"slope_AUC_mean_NDVI\", \"slope_AUC_mean_EVI\",\n",
    "    \"slope_harv_mean_NDVI\", \"slope_harv_mean_EVI\",\n",
    "    \"avg_clear_frac_harv_2022_2025\"\n",
    "]:\n",
    "    if col not in summary.columns:\n",
    "        summary[col] = np.nan\n",
    "\n",
    "# ---- Scoring weights (tweak as desired) ----\n",
    "W_LEVEL = {\n",
    "    # historical level: emphasize long-season productivity (AUC), include harvest means\n",
    "    \"base_AUC_NDVI\": 1.0,\n",
    "    \"base_AUC_EVI\":  1.0,\n",
    "    \"base_harv_NDVI\": 0.5,\n",
    "    \"base_harv_EVI\":  0.5,\n",
    "}\n",
    "W_MOMENTUM = {\n",
    "    # anomalies 2022–2025 (bigger is better)\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\": 1.0,\n",
    "    \"avg_AUC_EVI_anom_2022_2025\":  1.0,\n",
    "    \"avg_harv_NDVI_anom_2022_2025\": 0.5,\n",
    "    \"avg_harv_EVI_anom_2022_2025\":  0.5,\n",
    "    # slope (per year): emphasize AUC slopes, include harvest slopes\n",
    "    \"slope_AUC_mean_NDVI\": 0.75,\n",
    "    \"slope_AUC_mean_EVI\":  0.75,\n",
    "    \"slope_harv_mean_NDVI\": 0.5,\n",
    "    \"slope_harv_mean_EVI\":  0.5,\n",
    "}\n",
    "\n",
    "ALPHA_LEVEL = 0.5   # weight for Level vs Momentum in combined score\n",
    "ALPHA_MOM   = 0.5\n",
    "\n",
    "# QA weighting: downweight low clear fraction (range ~0.5..1)\n",
    "# If you prefer no QA weighting, set BETA_QA=0\n",
    "BETA_QA = 1.0\n",
    "def qa_weight(cf):\n",
    "    cf = pd.to_numeric(cf, errors=\"coerce\").fillna(1.0)\n",
    "    return 0.5 + 0.5 * cf  # 0.5 (low certainty) .. 1.0 (high clarity)\n",
    "\n",
    "# ---- Build standardized components ----\n",
    "lvl_terms = []\n",
    "for col, w in W_LEVEL.items():\n",
    "    z = zscore_winsor(summary[col])\n",
    "    lvl_terms.append(w * z)\n",
    "summary[\"LevelScore\"] = np.sum(lvl_terms, axis=0)\n",
    "\n",
    "mom_terms = []\n",
    "for col, w in W_MOMENTUM.items():\n",
    "    z = zscore_winsor(summary[col])\n",
    "    mom_terms.append(w * z)\n",
    "summary[\"MomentumScore\"] = np.sum(mom_terms, axis=0)\n",
    "\n",
    "# Combined & QA-weighted scores\n",
    "summary[\"CombinedScore\"] = ALPHA_LEVEL * summary[\"LevelScore\"] + ALPHA_MOM * summary[\"MomentumScore\"]\n",
    "summary[\"ClearWeight\"]   = qa_weight(summary[\"avg_clear_frac_harv_2022_2025\"])\n",
    "summary[\"WeightedCombined\"] = summary[\"CombinedScore\"] * (summary[\"ClearWeight\"] ** BETA_QA)\n",
    "\n",
    "# Ranks & percentiles (higher score = better rank)\n",
    "summary[\"Rank_Combined\"] = summary[\"CombinedScore\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_WeightedCombined\"] = summary[\"WeightedCombined\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Pct_WeightedCombined\"] = pct_rank(summary[\"WeightedCombined\"])\n",
    "\n",
    "# Also keep separate ranks for diagnostics\n",
    "summary[\"Rank_Level\"]    = summary[\"LevelScore\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_Momentum\"] = summary[\"MomentumScore\"].rank(ascending=False, method=\"min\")\n",
    "\n",
    "# Sort for leaderboard\n",
    "leaderboard = summary.sort_values(\"WeightedCombined\", ascending=False).reset_index(drop=True)\n",
    "leaderboard.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae0300-851d-4483-9edc-7807870fb305",
   "metadata": {},
   "source": [
    "### 3) Export overall leaderboard & quick printouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe4aaf-fc18-4666-b094-f6eb47d2fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_overall = OUT_DIR / \"adm2_rank_overall.csv\"\n",
    "leaderboard_cols = [\n",
    "    \"ADM2_PCODE\",\n",
    "    \"LevelScore\",\"Rank_Level\",\n",
    "    \"MomentumScore\",\"Rank_Momentum\",\n",
    "    \"CombinedScore\",\"Rank_Combined\",\n",
    "    \"ClearWeight\",\"WeightedCombined\",\"Rank_WeightedCombined\",\"Pct_WeightedCombined\",\n",
    "    # optional context columns:\n",
    "    \"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\",\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "    \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "    \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\",\n",
    "    \"avg_clear_frac_harv_2022_2025\"\n",
    "]\n",
    "leaderboard[leaderboard_cols].to_csv(out_overall, index=False)\n",
    "\n",
    "print(\"Top 10 (QA-weighted combined):\")\n",
    "print(leaderboard[[\"ADM2_PCODE\",\"WeightedCombined\",\"Rank_WeightedCombined\"]].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 (QA-weighted combined):\")\n",
    "print(leaderboard[[\"ADM2_PCODE\",\"WeightedCombined\",\"Rank_WeightedCombined\"]].tail(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nSaved overall ranking to:\", out_overall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9945aa-4afd-4dd0-aa4c-05d519d2950a",
   "metadata": {},
   "source": [
    "### 4) Per-year rankings (2019–2025) from the long “metrics” table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f47db5-a496-400e-a964-c7594cdbace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure needed columns exist\n",
    "for col in [\"AUC_mean_NDVI\",\"AUC_mean_EVI\",\"harv_mean_NDVI\",\"harv_mean_EVI\",\"year\"]:\n",
    "    if col not in metrics.columns:\n",
    "        raise ValueError(f\"Column '{col}' missing from metrics CSV.\")\n",
    "\n",
    "def per_year_score(df_year: pd.DataFrame) -> pd.Series:\n",
    "    # Standardize per-year across ADM2 so each year is comparable internally\n",
    "    z_auc_ndvi = zscore_winsor(df_year[\"AUC_mean_NDVI\"])\n",
    "    z_auc_evi  = zscore_winsor(df_year[\"AUC_mean_EVI\"])\n",
    "    z_h_ndvi   = zscore_winsor(df_year[\"harv_mean_NDVI\"])\n",
    "    z_h_evi    = zscore_winsor(df_year[\"harv_mean_EVI\"])\n",
    "    # Emphasize AUC, include harvest means\n",
    "    return 1.0*z_auc_ndvi + 1.0*z_auc_evi + 0.5*z_h_ndvi + 0.5*z_h_evi\n",
    "\n",
    "rows = []\n",
    "for yr, dfy in metrics.groupby(\"year\"):\n",
    "    dfy = dfy.copy()\n",
    "    dfy[\"YearScore\"] = per_year_score(dfy)\n",
    "    dfy[\"YearRank\"]  = dfy[\"YearScore\"].rank(ascending=False, method=\"min\")\n",
    "    dfy[\"YearPct\"]   = pct_rank(dfy[\"YearScore\"])\n",
    "    rows.append(dfy[[\"ADM2_PCODE\",\"year\",\"YearScore\",\"YearRank\",\"YearPct\"]])\n",
    "\n",
    "yearly_rank = pd.concat(rows, ignore_index=True).sort_values([\"year\",\"YearRank\"])\n",
    "out_yearly = OUT_DIR / \"adm2_rank_by_year.csv\"\n",
    "yearly_rank.to_csv(out_yearly, index=False)\n",
    "\n",
    "print(\"Saved per-year rankings to:\", out_yearly)\n",
    "yearly_rank.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bfdcab-a805-4bcb-b375-68c76187ec78",
   "metadata": {},
   "source": [
    "### 5) Compact report table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a911cea-946e-4d8e-866a-39a23bee332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Export full (all-ADM2) category rankings + updated workbook\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Resolve dirs & (re)load if needed ---\n",
    "if 'OUTPUT_DIR' in globals():\n",
    "    OUT_DIR = OUTPUT_DIR\n",
    "else:\n",
    "    OUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = OUT_DIR / \"panel_summary_by_ADM2.csv\"\n",
    "METRICS_CSV = OUT_DIR / \"panel_yearly_metrics_long.csv\"\n",
    "\n",
    "if 'summary' not in globals() or not isinstance(summary, pd.DataFrame):\n",
    "    summary = pd.read_csv(SUMMARY_CSV)\n",
    "if 'metrics' not in globals() or not isinstance(metrics, pd.DataFrame):\n",
    "    metrics = pd.read_csv(METRICS_CSV)\n",
    "\n",
    "# --- Helper for CV & winsor z (small, local versions) ---\n",
    "def _cv(series: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) < 3 or s.mean() == 0:\n",
    "        return np.nan\n",
    "    return float(s.std(ddof=0) / s.mean())\n",
    "\n",
    "def zscore_winsor(s: pd.Series, p: float = 0.05) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.notna().sum() < 2:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    lo, hi = s.quantile([p, 1-p])\n",
    "    s_clip = s.clip(lo, hi)\n",
    "    std = s_clip.std(ddof=0)\n",
    "    if std == 0 or np.isnan(std):\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    return (s_clip - s_clip.mean()) / std\n",
    "\n",
    "# --- Build CV table (for stability) if needed ---\n",
    "cv_all = (metrics\n",
    "          .groupby(\"ADM2_PCODE\")\n",
    "          .agg(cv_auc_ndvi=(\"AUC_mean_NDVI\", _cv),\n",
    "               cv_auc_evi =(\"AUC_mean_EVI\",  _cv))\n",
    "          .reset_index())\n",
    "cv_all[\"cv_auc_mean\"] = cv_all[[\"cv_auc_ndvi\",\"cv_auc_evi\"]].mean(axis=1, skipna=True)\n",
    "\n",
    "# --- Full category tables (ALL ADM2s; sorted + ranks) ---\n",
    "overall_leaders_full = (\n",
    "    summary\n",
    "    .assign(OverallRank=lambda d: d[\"WeightedCombined\"].rank(ascending=False, method=\"min\"))\n",
    "    .sort_values([\"WeightedCombined\"], ascending=False)\n",
    "    .loc[:, [\"ADM2_PCODE\",\"OverallRank\",\"WeightedCombined\",\"ClearWeight\",\n",
    "             \"CombinedScore\",\"LevelScore\",\"MomentumScore\"]]\n",
    ")\n",
    "\n",
    "top_baseline_full = (\n",
    "    summary\n",
    "    .assign(BaselineRank=lambda d: d[\"LevelScore\"].rank(ascending=False, method=\"min\"))\n",
    "    .sort_values([\"LevelScore\"], ascending=False)\n",
    "    .loc[:, [\"ADM2_PCODE\",\"BaselineRank\",\"LevelScore\",\n",
    "             \"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\"]]\n",
    ")\n",
    "\n",
    "top_improvers_full = (\n",
    "    summary\n",
    "    .assign(ImproverRank=lambda d: d[\"MomentumScore\"].rank(ascending=False, method=\"min\"))\n",
    "    .sort_values([\"MomentumScore\"], ascending=False)\n",
    "    .loc[:, [\"ADM2_PCODE\",\"ImproverRank\",\"MomentumScore\",\n",
    "             \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "             \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "             \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\n",
    "             \"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\"]]\n",
    ")\n",
    "\n",
    "most_stable_full = (\n",
    "    cv_all\n",
    "    .assign(StabilityRank=lambda d: d[\"cv_auc_mean\"].rank(ascending=True, method=\"min\"))\n",
    "    .sort_values([\"cv_auc_mean\",\"cv_auc_ndvi\",\"cv_auc_evi\"], ascending=[True, True, True])\n",
    "    .loc[:, [\"ADM2_PCODE\",\"StabilityRank\",\"cv_auc_mean\",\"cv_auc_ndvi\",\"cv_auc_evi\"]]\n",
    ")\n",
    "\n",
    "# --- Year winners (keep top-5 per year for narrative) ---\n",
    "if 'year_winners' not in globals():\n",
    "    rows = []\n",
    "    for yr, dfy in metrics.groupby(\"year\"):\n",
    "        dfy = dfy.copy()\n",
    "        z_auc_ndvi = zscore_winsor(dfy[\"AUC_mean_NDVI\"])\n",
    "        z_auc_evi  = zscore_winsor(dfy[\"AUC_mean_EVI\"])\n",
    "        z_h_ndvi   = zscore_winsor(dfy[\"harv_mean_NDVI\"])\n",
    "        z_h_evi    = zscore_winsor(dfy[\"harv_mean_EVI\"])\n",
    "        dfy[\"YearScore\"] = 1.0*z_auc_ndvi + 1.0*z_auc_evi + 0.5*z_h_ndvi + 0.5*z_h_evi\n",
    "        dfy[\"YearRank\"]  = dfy[\"YearScore\"].rank(ascending=False, method=\"min\")\n",
    "        rows.append(dfy[[\"ADM2_PCODE\",\"year\",\"YearScore\",\"YearRank\"]])\n",
    "    year_winners = (pd.concat(rows, ignore_index=True)\n",
    "                    .sort_values([\"year\",\"YearRank\"])\n",
    "                    .groupby(\"year\").head(5).reset_index(drop=True))\n",
    "\n",
    "# --- Compact report (same as before; keeps all ADM2s) ---\n",
    "compact_cols = [\n",
    "    \"ADM2_PCODE\",\n",
    "    \"Rank_WeightedCombined\",\"WeightedCombined\",\"ClearWeight\",\n",
    "    \"Rank_Level\",\"LevelScore\",\n",
    "    \"Rank_Momentum\",\"MomentumScore\",\n",
    "    \"base_AUC_NDVI\",\"base_AUC_EVI\",\"base_harv_NDVI\",\"base_harv_EVI\",\n",
    "    \"avg_AUC_NDVI_anom_2022_2025\",\"avg_AUC_EVI_anom_2022_2025\",\n",
    "    \"avg_harv_NDVI_anom_2022_2025\",\"avg_harv_EVI_anom_2022_2025\",\n",
    "    \"slope_AUC_mean_NDVI\",\"slope_AUC_mean_EVI\",\"slope_harv_mean_NDVI\",\"slope_harv_mean_EVI\",\n",
    "]\n",
    "missing_compact = [c for c in compact_cols if c not in summary.columns]\n",
    "if missing_compact:\n",
    "    print(\"Note: some compact-report columns missing; output will include what's available:\", missing_compact)\n",
    "compact = (summary.loc[:, [c for c in compact_cols if c in summary.columns]]\n",
    "           .merge(cv_all, on=\"ADM2_PCODE\", how=\"left\")\n",
    "           .sort_values([\"Rank_WeightedCombined\",\"Rank_Level\"], na_position=\"last\")\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "# --- Save workbook (overwrites existing to include full lists) ---\n",
    "out_xlsx = OUT_DIR / \"adm2_category_rankings.xlsx\"\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as xw:\n",
    "    overall_leaders_full.to_excel(xw, sheet_name=\"Overall_Leaders\", index=False)\n",
    "    top_baseline_full.to_excel(xw, sheet_name=\"Top_Baseline\", index=False)\n",
    "    top_improvers_full.to_excel(xw, sheet_name=\"Top_Improvers\", index=False)\n",
    "    most_stable_full.to_excel(xw, sheet_name=\"Most_Stable\", index=False)\n",
    "    year_winners.to_excel(xw, sheet_name=\"Year_Winners\", index=False)\n",
    "    compact.to_excel(xw, sheet_name=\"Compact_Report\", index=False)\n",
    "\n",
    "# --- Also save individual CSVs for convenience ---\n",
    "(overall_leaders_full\n",
    " ).to_csv(OUT_DIR / \"adm2_overall_leaders.csv\", index=False)\n",
    "(top_baseline_full\n",
    " ).to_csv(OUT_DIR / \"adm2_top_baseline.csv\", index=False)\n",
    "(top_improvers_full\n",
    " ).to_csv(OUT_DIR / \"adm2_top_improvers.csv\", index=False)\n",
    "(most_stable_full\n",
    " ).to_csv(OUT_DIR / \"adm2_most_stable.csv\", index=False)\n",
    "\n",
    "print(\"Saved full-category outputs to:\")\n",
    "print(\" -\", out_xlsx)\n",
    "print(\" -\", OUT_DIR / \"adm2_overall_leaders.csv\")\n",
    "print(\" -\", OUT_DIR / \"adm2_top_baseline.csv\")\n",
    "print(\" -\", OUT_DIR / \"adm2_top_improvers.csv\")\n",
    "print(\" -\", OUT_DIR / \"adm2_most_stable.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460dc31-b550-449b-9857-cf80a46754a8",
   "metadata": {},
   "source": [
    "## 3) Seasonal graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2339099-3359-4346-86b7-04eecd566287",
   "metadata": {},
   "source": [
    "### Output 'seasonal_overlay_ndvi_estharvest'/'seasonal_overlay_evi_estharvest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd392d-26ec-4790-9100-6e8f5eaba42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Recreate seasonal overlays using estimated harvest months (per ADM2)\n",
    "# - NDVI overlay shades NDVI-estimated window\n",
    "# - EVI  overlay shades EVI-estimated window\n",
    "# ======================================================\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Paths ----------\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "if 'PLOTS_DIR' not in globals():\n",
    "    PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Monthly CSV location (fallback if `data` is missing)\n",
    "if 'INPUT_DIR' in globals():\n",
    "    MONTHLY_DIR = Path(INPUT_DIR)\n",
    "else:\n",
    "    MONTHLY_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi\")\n",
    "\n",
    "HARVEST_CSV = OUTPUT_DIR / \"estimated_harvest_windows.csv\"\n",
    "if not HARVEST_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing {HARVEST_CSV}. Run the harvest-estimation cell first.\")\n",
    "\n",
    "# Output folders (new, so originals remain)\n",
    "NDVI_DIR = PLOTS_DIR / \"seasonal_overlay_ndvi_estharvest\"\n",
    "EVI_DIR  = PLOTS_DIR / \"seasonal_overlay_evi_estharvest\"\n",
    "NDVI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Config ----------\n",
    "BASELINE_YEARS = [2019, 2020, 2021]\n",
    "Y_LIMIT = (0, 1)  # consistent axis 0..1\n",
    "MONTH_LABELS = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "# ---------- Load monthly data if needed ----------\n",
    "def _read_one_csv(fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'date' column in {fp}\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    # ADM2 from filename if absent\n",
    "    adm2_guess = Path(fp).name.split(\"_\")[0]\n",
    "    if \"ADM2_PCODE\" not in df.columns:\n",
    "        df[\"ADM2_PCODE\"] = adm2_guess\n",
    "    else:\n",
    "        df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].fillna(adm2_guess)\n",
    "    # Coerce key numeric fields\n",
    "    for col in [\"mean_NDVI\",\"mean_EVI\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _load_monthly_if_needed():\n",
    "    if 'data' in globals():\n",
    "        need = {\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"}\n",
    "        if isinstance(data, pd.DataFrame) and need.issubset(data.columns):\n",
    "            return data\n",
    "    files = glob.glob(str(MONTHLY_DIR / \"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No monthly CSVs found in {MONTHLY_DIR}\")\n",
    "    frames = []\n",
    "    for fp in tqdm(files, desc=\"Loading monthly CSVs\"):\n",
    "        try:\n",
    "            frames.append(_read_one_csv(fp))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No valid monthly CSVs loaded.\")\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "data = _load_monthly_if_needed()\n",
    "\n",
    "# ---------- Harvest windows ----------\n",
    "harvest = pd.read_csv(HARVEST_CSV)\n",
    "need_hw = {\"ADM2_PCODE\",\"ndvi_start_month\",\"ndvi_end_month\",\"evi_start_month\",\"evi_end_month\"}\n",
    "if not need_hw.issubset(harvest.columns):\n",
    "    raise ValueError(f\"{HARVEST_CSV} missing columns: {need_hw - set(harvest.columns)}\")\n",
    "\n",
    "# ---------- Plot helper ----------\n",
    "def _shade_window(start_m: int, end_m: int):\n",
    "    \"\"\"Return list of (x0, x1) spans in month-number space for shading, handling wrap-around.\"\"\"\n",
    "    if pd.isna(start_m) or pd.isna(end_m):\n",
    "        return []\n",
    "    start_m = int(start_m); end_m = int(end_m)\n",
    "    if 1 <= start_m <= 12 and 1 <= end_m <= 12:\n",
    "        if start_m <= end_m:\n",
    "            return [(start_m - 0.5, end_m + 0.5)]\n",
    "        else:\n",
    "            return [(0.5, end_m + 0.5), (start_m - 0.5, 12.5)]\n",
    "    return []\n",
    "\n",
    "def _baseline_line(piv, baseline_years):\n",
    "    cols = [y for y in baseline_years if y in piv.columns]\n",
    "    return piv[cols].mean(axis=1) if cols else None\n",
    "\n",
    "def _seasonal_overlay(df_adm2: pd.DataFrame, value_col: str, start_m: int, end_m: int, out_path: Path):\n",
    "    # Pivot: rows=month(1..12), cols=year\n",
    "    months = np.arange(1, 13, dtype=int)\n",
    "    piv = (df_adm2.pivot_table(index=\"month\", columns=\"year\", values=value_col, aggfunc=\"mean\")\n",
    "           .reindex(index=months))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    # lines for each year\n",
    "    for y in sorted([c for c in piv.columns if pd.notna(c)]):\n",
    "        plt.plot(months, piv[y].values, label=str(int(y)))\n",
    "    # baseline\n",
    "    base = _baseline_line(piv, BASELINE_YEARS)\n",
    "    if base is not None:\n",
    "        plt.plot(months, base.values, linewidth=3, label=\"Baseline (2019–2021)\")\n",
    "\n",
    "    # shade harvest\n",
    "    for x0, x1 in _shade_window(start_m, end_m):\n",
    "        plt.axvspan(x0, x1, alpha=0.12)\n",
    "\n",
    "    # cosmetics\n",
    "    plt.xticks(months, MONTH_LABELS)\n",
    "    plt.ylim(*Y_LIMIT)\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(value_col)\n",
    "    adm2 = str(df_adm2[\"ADM2_PCODE\"].iloc[0])\n",
    "    metric = \"NDVI\" if \"NDVI\" in value_col else \"EVI\"\n",
    "    if not (pd.isna(start_m) or pd.isna(end_m)):\n",
    "        title_h = f\" (est. harvest {MONTH_LABELS[start_m-1]}–{MONTH_LABELS[end_m-1]})\"\n",
    "    else:\n",
    "        title_h = \" (no estimate)\"\n",
    "    plt.title(f\"{adm2} — Seasonal overlay ({metric}){title_h}\")\n",
    "    plt.legend(ncols=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Generate overlays with estimated shading ----------\n",
    "adm2_list = sorted(data[\"ADM2_PCODE\"].dropna().unique().tolist())\n",
    "hw_map = harvest.set_index(\"ADM2_PCODE\").to_dict(orient=\"index\")\n",
    "\n",
    "for adm2 in tqdm(adm2_list, desc=\"Recreating overlays with estimated harvest\"):\n",
    "    sub = data.loc[data[\"ADM2_PCODE\"] == adm2, [\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"]].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    # pick windows (NDVI plot uses NDVI window; EVI plot uses EVI window)\n",
    "    hw = hw_map.get(adm2, {})\n",
    "    ndvi_s = hw.get(\"ndvi_start_month\", np.nan); ndvi_e = hw.get(\"ndvi_end_month\", np.nan)\n",
    "    evi_s  = hw.get(\"evi_start_month\",  np.nan); evi_e  = hw.get(\"evi_end_month\",  np.nan)\n",
    "\n",
    "    # NDVI overlay\n",
    "    _seasonal_overlay(\n",
    "        sub[[\"ADM2_PCODE\",\"year\",\"month\",\"mean_NDVI\"]].dropna(subset=[\"mean_NDVI\"]),\n",
    "        \"mean_NDVI\",\n",
    "        ndvi_s, ndvi_e,\n",
    "        NDVI_DIR / f\"{adm2}_seasonal_overlay_ndvi.png\"\n",
    "    )\n",
    "    # EVI overlay\n",
    "    _seasonal_overlay(\n",
    "        sub[[\"ADM2_PCODE\",\"year\",\"month\",\"mean_EVI\"]].dropna(subset=[\"mean_EVI\"]),\n",
    "        \"mean_EVI\",\n",
    "        evi_s, evi_e,\n",
    "        EVI_DIR / f\"{adm2}_seasonal_overlay_evi.png\"\n",
    "    )\n",
    "\n",
    "print(\"Done. Saved updated overlays to:\")\n",
    "print(\" -\", NDVI_DIR)\n",
    "print(\" -\", EVI_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c6a81-9b2c-462a-8469-c93438547bec",
   "metadata": {},
   "source": [
    "### Output 'timeseries_ndvi_estharvest'/'timeseries_evi_estharvest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd178f80-e2e8-4504-8c9c-9b1d49e12b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Non-overlaid monthly time series with per-year harvest shading\n",
    "# ======================================================\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, MonthLocator, YearLocator\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Paths ----------\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    OUTPUT_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi_outputs\")\n",
    "if 'PLOTS_DIR' not in globals():\n",
    "    PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NDVI_TS_DIR = PLOTS_DIR / \"timeseries_ndvi_estharvest\"\n",
    "EVI_TS_DIR  = PLOTS_DIR / \"timeseries_evi_estharvest\"\n",
    "NDVI_TS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVI_TS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Monthly CSV location if `data` is missing\n",
    "if 'INPUT_DIR' in globals():\n",
    "    MONTHLY_DIR = Path(INPUT_DIR)\n",
    "else:\n",
    "    MONTHLY_DIR = Path(r\"C:\\temp\\timor_leste\\ndvi_evi\")\n",
    "\n",
    "HARVEST_CSV = OUTPUT_DIR / \"estimated_harvest_windows.csv\"\n",
    "if not HARVEST_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing {HARVEST_CSV}. Run the harvest-estimation cell first.\")\n",
    "harvest_df = pd.read_csv(HARVEST_CSV)\n",
    "\n",
    "# ---------- Load monthly data if needed ----------\n",
    "def _read_one_csv(fp: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'date' column in {fp}\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"]  = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    # ADM2 from filename if absent\n",
    "    adm2_guess = Path(fp).name.split(\"_\")[0]\n",
    "    if \"ADM2_PCODE\" not in df.columns:\n",
    "        df[\"ADM2_PCODE\"] = adm2_guess\n",
    "    else:\n",
    "        df[\"ADM2_PCODE\"] = df[\"ADM2_PCODE\"].fillna(adm2_guess)\n",
    "    # Coerce useful numeric fields\n",
    "    for col in [\"mean_NDVI\",\"mean_EVI\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _load_monthly_if_needed():\n",
    "    if 'data' in globals():\n",
    "        need = {\"ADM2_PCODE\",\"date\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"}\n",
    "        if isinstance(data, pd.DataFrame) and need.issubset(data.columns):\n",
    "            return data\n",
    "    files = glob.glob(str(MONTHLY_DIR / \"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No monthly CSVs found in {MONTHLY_DIR}\")\n",
    "    frames = []\n",
    "    for fp in tqdm(files, desc=\"Loading monthly CSVs\"):\n",
    "        try:\n",
    "            frames.append(_read_one_csv(fp))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fp}: {e}\")\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No valid monthly CSVs loaded.\")\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "data = _load_monthly_if_needed()\n",
    "\n",
    "# Safety check\n",
    "need_cols = {\"ADM2_PCODE\",\"date\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"}\n",
    "missing = need_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Monthly data missing columns: {missing}.\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "MONTH_NAMES = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "def _eom(year: int, month: int) -> datetime:\n",
    "    \"\"\"End-of-month date.\"\"\"\n",
    "    day = calendar.monthrange(year, month)[1]\n",
    "    return datetime(year, month, day)\n",
    "\n",
    "def _shades_for_year(y: int, start_m: int, end_m: int):\n",
    "    \"\"\"\n",
    "    Return list of (start_date, end_date) spans within calendar year y\n",
    "    to shade, given a start/end month (handles wrap).\n",
    "    \"\"\"\n",
    "    if pd.isna(start_m) or pd.isna(end_m):\n",
    "        return []\n",
    "    start_m = int(start_m); end_m = int(end_m)\n",
    "    spans = []\n",
    "    if 1 <= start_m <= 12 and 1 <= end_m <= 12:\n",
    "        if start_m <= end_m:\n",
    "            spans.append((datetime(y, start_m, 1), _eom(y, end_m)))\n",
    "        else:\n",
    "            # wrap: shade Jan..end_m and start_m..Dec inside the SAME calendar year\n",
    "            spans.append((datetime(y, 1, 1), _eom(y, end_m)))\n",
    "            spans.append((datetime(y, start_m, 1), datetime(y, 12, 31)))\n",
    "    return spans\n",
    "\n",
    "def _plot_timeseries_with_shading(df_adm2: pd.DataFrame,\n",
    "                                  value_col: str,\n",
    "                                  start_m: int, end_m: int,\n",
    "                                  out_path: Path):\n",
    "    dfp = df_adm2.sort_values(\"date\")\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(dfp[\"date\"], dfp[value_col], marker=\"o\", linewidth=1.5, label=value_col)\n",
    "\n",
    "    # Per-year shading using estimated window months\n",
    "    for y in sorted(dfp[\"year\"].unique()):\n",
    "        for d0, d1 in _shades_for_year(y, start_m, end_m):\n",
    "            # limit to plotting range\n",
    "            left  = max(d0, dfp[\"date\"].min().to_pydatetime())\n",
    "            right = min(d1, dfp[\"date\"].max().to_pydatetime())\n",
    "            if left <= right:\n",
    "                ax.axvspan(left, right, alpha=0.12)\n",
    "\n",
    "    # Cosmetics\n",
    "    ax.set_ylim(0, 1)  # consistent scale\n",
    "    ax.set_ylabel(value_col)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    adm2 = str(dfp[\"ADM2_PCODE\"].iloc[0])\n",
    "    if not (pd.isna(start_m) or pd.isna(end_m)):\n",
    "        title_h = f\" (est. harvest {MONTH_NAMES[start_m-1]}–{MONTH_NAMES[end_m-1]})\"\n",
    "    else:\n",
    "        title_h = \" (no harvest estimate)\"\n",
    "    ax.set_title(f\"{adm2} — Monthly time series {title_h}\")\n",
    "\n",
    "    # Ticks/formatters\n",
    "    ax.xaxis.set_major_locator(YearLocator())\n",
    "    ax.xaxis.set_minor_locator(MonthLocator(bymonth=[1,4,7,10]))\n",
    "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n",
    "    ax.grid(True, axis=\"y\", alpha=0.2)\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- Build maps for quick lookup ----------\n",
    "hw = harvest_df.set_index(\"ADM2_PCODE\").to_dict(orient=\"index\")\n",
    "\n",
    "# ---------- Generate plots ----------\n",
    "adm2_list = sorted(data[\"ADM2_PCODE\"].dropna().unique().tolist())\n",
    "\n",
    "for adm2 in tqdm(adm2_list, desc=\"Time series with harvest shading\"):\n",
    "    sub = data.loc[data[\"ADM2_PCODE\"] == adm2, [\"ADM2_PCODE\",\"date\",\"year\",\"month\",\"mean_NDVI\",\"mean_EVI\"]].dropna(subset=[\"date\"]).copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    # Ensure datetime dtype\n",
    "    sub[\"date\"] = pd.to_datetime(sub[\"date\"])\n",
    "\n",
    "    # NDVI window\n",
    "    ndvi_s = hw.get(adm2, {}).get(\"ndvi_start_month\", np.nan)\n",
    "    ndvi_e = hw.get(adm2, {}).get(\"ndvi_end_month\",   np.nan)\n",
    "    _plot_timeseries_with_shading(sub[[\"ADM2_PCODE\",\"date\",\"year\",\"mean_NDVI\"]].dropna(subset=[\"mean_NDVI\"]),\n",
    "                                  \"mean_NDVI\", ndvi_s, ndvi_e,\n",
    "                                  NDVI_TS_DIR / f\"{adm2}_timeseries_ndvi.png\")\n",
    "\n",
    "    # EVI window\n",
    "    evi_s  = hw.get(adm2, {}).get(\"evi_start_month\",  np.nan)\n",
    "    evi_e  = hw.get(adm2, {}).get(\"evi_end_month\",    np.nan)\n",
    "    _plot_timeseries_with_shading(sub[[\"ADM2_PCODE\",\"date\",\"year\",\"mean_EVI\"]].dropna(subset=[\"mean_EVI\"]),\n",
    "                                  \"mean_EVI\",  evi_s, evi_e,\n",
    "                                  EVI_TS_DIR  / f\"{adm2}_timeseries_evi.png\")\n",
    "\n",
    "print(\"Saved time-series plots to:\")\n",
    "print(\" -\", NDVI_TS_DIR)\n",
    "print(\" -\", EVI_TS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
